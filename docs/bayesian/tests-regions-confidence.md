# Testes e regi√µes de confian√ßa

Com a abordagem bayesiana, podemos fazer afirma√ß√µes do tipo $\pi(\theta \in \Theta_0 | x)$, o que √© difere da proposta frequentista, j√° que l√° o par√¢metro √© fixo, apesar de desconhecido. 
Alguns bayesianos acreditam que o teste, em especial o teste com hip√≥tese nula pontual, n√£o deveria ser realizado por diversos motivos: representa√ß√£o redutiva do modelo; priori modificada de forma n√£o natural para representar a quest√£o pontual; falta de estrutura baseada em teoria da decis√£o; impossibilidade do uso de prioris impr√≥prias.

## Uma primeira abordagem

Considere um modelo $f(x|\theta)$ com $\theta \in \Theta$. 
Queremos verificar $H_0 : \theta \in \Theta_0$ com $\Theta_0 \subseteq \Theta$ sendo um subconjunto de interesse.
Chamamos $H_0$ de **hip√≥tese nula**. 
Em contrapartida, temos a **hip√≥tese alternativa** $H_1 : \theta \in \Theta_1$ com $\Theta_0 \cap \Theta_1 = \emptyset$ e $\theta$ pertencendo a um dos dois conjuntos.

A perda proposta por Neyman e Pearson √© 
$$
L(\theta, \varphi) = \begin{cases}
    1 &\text{se } \varphi \neq 1_{\Theta_0}(\theta), \\
    0 &\text{c.c.}
\end{cases}
$$
Para essa perda, o estimador de Bayes √© 
$$
\varphi^{\pi}(x) = \begin{cases}
    1 &\text{se } \Pr(\theta \in \Theta_0 | x) > \Pr(\theta \in \Theta_1 | x), \\
    0 &\text{c.c.}
\end{cases}
$$
Podemos generalizar essa perda para
$$
L(\theta, \varphi) = \begin{cases}
    0 &\text{se } \varphi = 1_{\Theta_0}(\theta), \\
    a_0 &\text{se } \theta \in \Theta_0 \text{ e } \varphi = 0, \\
    a_1 &\text{se } \theta \in \Theta_1 \text{ e } \varphi = 1, 
\end{cases}
$$
cujo estimador de Bayes √© $\varphi^{\pi}(x) = 1$ se $\Pr(\theta \in \Theta_0 | x) > a_1/(a_0 + a_1)$, sendo essa fra√ß√£o o **n√≠vel de aceita√ß√£o**.

### Fator de Bayes

Definimos o fator de Bayes como a raz√£o das raz√µes da posteriori e da priori:
$$
B_{01}^{\pi}(x) = \frac{\Pr(\theta \in \Theta_0 | x)}{\Pr(\theta \in \Theta_1 | x)}\Big /\frac{\Pr(\theta \in \Theta_0)}{\Pr(\theta \in \Theta_1)} = \frac{\int_{\Theta_0} f(x|\theta)\pi_0(\theta) \, d\theta}{\int_{\Theta_1} f(x|\theta)\pi_1(\theta) \, d\theta},
$$
que avalia a modifica√ß√£o das chances de $\Theta_0$ contra $\Theta_1$. 
Observe que $\pi_i$ √© a distribui√ß√£o a priori sob $H_i$.

Jeffreys desenvolveu uma escala (fora da uma configura√ß√£o baseada em teoria da decis√£o) para avaliar a evid√™ncia trazida por $x$ contra $H_0$, considerando o valor $\log_{10}(B_{10}^{\pi})$: 
$$0 < \text{ pobre } < 0.5 < \text{ substancial } < 1 < \text{ forte } < 2 < \text{ decisiva } < +\infty.$$

---
``üìù`` **Exemplo (Basquete)**

Um jogador de basquete tem "m√£o quente" quando tem bons e maus dias, ao inv√©s de uma probabilidade fixa de vencer em um lan√ßamento.
O modelo base √© $H_0: y_i \sim B(n_i, p)$ para cada jogo $i=1,\dots, G$ ($p$ √© fixo). 
A Alternativa √© que $p$ varia com o dia.
Considere a priori $p_i \sim Beta(\xi/\omega, (1-\xi)/\omega)$, de forma que $\xi = \mathbb{E}[p_i|\xi] \sim Unif(0,1)$. 
Isto √©, estamos fixando as m√©dias das jogadas dos dias e afirmando que alguns dias teremos $p_i > \xi$ e outros $p_i < \xi$, √© claro com alguma probabilidade. 
Al√©m do mais, consideramos $p \sim Unif(0,1)$. 
O fator de Bayes √© 
$$
B_{10} = \frac{\int_0^1 \left\{\prod_{i=1}^G \int_0^1 p_i^{y_i}(1-p_i)^{n_i-y_i}p_i^{\alpha-1}(1-p_i)^{\beta-1} \, dp_i \right\}\left(\Gamma(1/\omega)/\Gamma(\xi/\omega)\Gamma((1-\xi)/\omega)\right)^G \, d\xi}{\int_0^1 p^{\sum y_i}(1-p)^{\sum (n_i-y_i)} \, dp}
$$

No paper [Kass, Raftery (1995)](https://sites.stat.washington.edu/raftery/Research/PDF/kass1995.pdf), verificou-se que para $\omega = 0.005$, temos $B_{10} = 0.16$, que d√° baixa evid√™ncia a favor de $H_1$, isto √©, que existem bons e maus dias. Escolhemos $\omega$ olhando para $Var(p_i | \xi) = \xi(1-\xi)/(1+\omega^{-1}) \implies \omega^{-1} = \xi(1-\xi)/Var(p_i) - 1.$

---

Note que se $H_0$ for pontual, n√£o conseguimos usar o Fator de Bayes. Para esses casos, precisamos definir 
$$
\pi(\theta) = \Pr(\theta \in \Theta_0) \pi_{0}(\theta) + \Pr(\theta \in \Theta_1) \pi_{1}(\theta) = \varrho_0 \pi_0(\theta) + \varrho_1 \pi_1(\theta).
$$
Apesar de hip√≥teses pontuais serem problem√°ticas, elas t√™m muita utilidade na pr√°tica. 

Assim, teremos a priori $\pi(\theta) = \varrho_0 1_{\theta = \theta_0} + (1-\varrho_0)g_1(\theta)$ com posteriori
$$
\pi(\Theta_0 | x) = \frac{f(x|\theta_0) \varrho_0}{f(x|\theta_0)\varrho_0 + (1-\varrho_0)m_1(x))},
$$
em que $m_1$ √© a marginal dos dados sob $H_1$. Em particular, o Fator de Bayes √© 
$$
B_{01}^{\pi}(x) = \frac{f(x|\theta_0)}{m_1(x)},
$$
levando √† rela√ß√£o
$$
\pi(\Theta_0 | x) = \left[1 + \frac{1-\varrho_0}{\varrho_0} \frac{1}{B_{01}(x)}\right]^{-1}.
$$

---
``üìù`` **Exemplo (Distribui√ß√£o normal)**

Considere $x \sim \operatorname{Normal}(\theta, 1)$ e $H_0: \theta = 0$. 
Se usarmos a priori impr√≥pria $\pi(\theta) = \frac{1}{2}1_{\theta = 0} + \frac{1}{2}$, teremos que a posteriori de $H_0$ ser√° 
$$
\pi(\theta = 0 | x) = \frac{e^{-x^2/2}}{e^{-x^2/2} + \int e^{-(x-\theta)^2/2} \, d\theta} = \frac{1}{1 + \sqrt{2\pi}e^{x^2/2}} \le 0.285,
$$
isto √©, a posteriori √© limitada superiormente por um valor baixo. Fen√¥meno parecido acontece quando $\Theta_0$ √© um conjunto compacto. 

Uma quest√£o interessante √© que para os n√≠veis tradicionais, que ocorrem quando $x = 1.65, 1.96$ e $2.58$ (respectivamente n√≠veis $0.1, 0.05, 0.01$), a posteriori de $H_0$ √© pr√≥xima a esses valores (quando a priori √© a medida de Lebesgue).

Em particular, quando $H_0 : \theta \le 0$ para $\pi(\theta) = 1$, temos que 
$$
\pi(\theta \le 0 | x) = \Phi(-x),
$$
que √© o **p-valor** desse mesmo teste.

---

## Compara√ß√£o com a abordagem cl√°ssica

A abordagem cl√°ssica da teoria de testagem √© de Neyman-Pearson. Com isso, consideramos a seguinte defini√ß√£o:

**Poder:** O poder de um procedimento de teste $\varphi$ √© a probabilidade de rejeitar $H_0$ sob a hip√≥tese alternativa, isto √©, $\beta(\theta) = 1 - \mathbb{E}_{\theta}[\varphi(x)]$ quando $\theta \in \Theta_1$. A quantidade $1- \beta(\theta)$ √© o *erro do tipo II*, enquanto o *erro do tipo I* √© $1-\mathbb{E}_{\theta}[\varphi(x)]$ quando $\theta \in \Theta_0$. 
Em resumo, 

- $\theta \in \Theta_0 \implies 1-\mathbb{E}_{\theta}[\varphi(x)]$ √© erro do tipo I.
- $\theta \in \Theta_1 \implies \mathbb{E}_{\theta}[\varphi(x)]$ √© erro do tipo II.

### Testes UMP

Testes frequentistas buscam minimizar o risco frequentista $\mathbb{E}_{\theta}[L(\theta, \varphi(x))]$ sob $H_1$. 
Em particular, minimiza-se na classe de procesdimentos $C_{\alpha}$, em que $\alpha \in (0,1)$ e 
$$
C_{\alpha} = \{\varphi : \sup_{\theta \in \Theta_0} \mathbb{E}_{\theta}[L(\theta, \varphi(x))] =  \sup_{\theta \in \Theta_0} \Pr_{\theta}(\varphi(x) = 0) \le \alpha \}.
$$

Um teste $\varphi$ √© dito **Uniformemente Mais Poderoso (UMP)** a n√≠vel $\alpha$ se $\varphi \in C_{\alpha}$ e se ele minimiza o risco frequentista uniformemente em $\Theta_1$ em $C_{\alpha}$.
Esse m√©todo √© desbalanceado com respeito √†s hip√≥teses, porque o erro do tipo II pode ser muito grande, mesmo quando $\Theta_0$ √© uma transforma√ß√£o cont√≠nua de $\Theta_1$. 

**Proposi√ß√£o:** Seja $f(x|\theta)$ uma distribui√ß√£o que possua raz√£o de verossimilhan√ßa crescente em $T(x)$, isto √©, para $\theta_1 > \theta_2$, a fun√ß√£o $f(x|\theta_1) / f(x|\theta_2)$ cresce com $T(x)$. Se queremos testar $H_0 : \theta \le \theta_0$, existe um teste UMP da forma 
$$
\varphi(x) = \begin{cases}
	1 &\text{se } T(x) < c, \\
    \gamma &\text{se } T(x) = c, \\
    0 &\text{c.c.},    
\end{cases}
$$
de forma que $\mathbb{E}_{\theta_0}[\varphi(x)] = \alpha$. 

---
``üìù`` **Exemplo (Fam√≠lia exponencial)**

Seja $f(x|\theta)$ da fam√≠lia exponencial, isto √©, 
$$
\log \frac{f(x|\theta_1)}{f(x|\theta_2)} = \theta_1\cdot x - \psi(\theta_1) - \theta_2 \cdot x + \psi(\theta_2) = (\theta_1 - \theta_2)\cdot x - (\psi(\theta_1) - \psi(\theta_2)),
$$
que √© crescente em $T(x) = x$.

---

Podemos construir uma proposi√ß√£o no mesmo sentido da anterior, s√≥ que quando $H_0$ √© do tipo $\theta \not\in (\theta_1, \theta_2)$ e a densidade do dado pertence √† fam√≠lia exponencial. 
Nesse caso o teste UMP √© da forma $\varphi(x) = 1$ se $T(x) \not \in [c_1, c_2]$ e $\varphi(x) = \gamma_i$ se $T(x) = c_i$, em que $c_i$ e $\gamma_i$ s√£o escolhidos a partir da condi√ß√£o de que $\mathbb{E}_{\theta_i}[\varphi(x)] = \alpha$.
No caso em que o papel de $H_0$ √© trocado com o de $H_1$, n√£o existe teste UMP. 
Nessas situa√ß√µes, podemos restringir a classe de testes para os n√£o enviesados, em que
$$
\sup_{\Theta_0} \Pr_{\theta}(\varphi(x) = 0) \le \inf_{\Theta_1} \Pr_{\theta}(\varphi(x) = 0). 
$$
Isso leva a no√ß√£o de teste **Uniformemente Mais Poderoso n√£o enviesado (UMPU)**. 

Outra forma de construir testes √© baseada na distribui√ß√£o (em geral assint√≥tica) de 
$$
\frac{\sup_{\Theta_0} f(x|\theta)}{\sup_{\Theta_1} f(x|\theta)}.
$$

### p-valores

O p-valor associado a um teste √© o menor n√≠vel de signific√¢ncia $\alpha$ para o qual a hip√≥tese nula √© rejeitada.  
Para a hip√≥tese nula pontual, uma defini√ß√£o mais geral considera como p-valor qualquer estat√≠stica com distribui√ß√£o uniforme sob a hip√≥tese nula. 
Algumas cr√≠ticas: 

- O p-valor contradiz o Princ√≠pio da Verossimilhan√ßa, pois envolve a distribui√ß√£o inteira da observa√ß√£o;

- N√£o s√£o avaliados sob nenhum fun√ß√£o de perda, loho n√£o tem otimalidade intr√≠nseca;

- O p-valor √© frequentemente visto como uma aproxima√ß√£o para $\Pr(\theta \in \Theta_0|x)$, apesar de isso ser insignificante em uma configura√ß√£o frequentista.

- p-valores n√£o sumarizam toda a informa√ß√£o do teste, afinal o erro do tipo II √© omitido. Isso √© perigoso, afinal na pr√°tica o p-valor √© visto como o procedimento de teste. 

## Uma segunda abordagem

A ideia √© construir uma alternativa a Newyman-Pearson que justifique o uso de probabilidades a posteriori como medidas para testes.
A primeira modifica√ß√£o √© alterar o espa√ßo de decis√µes $\mathcal{D} = \{0,1\}$ para o intervalo $[0,1]$.
Queremos que eles indiquem o grau de evid√™ncia a favor de $H_0$.

**Proposi√ß√£o:** Sob a perda $L_2(\theta, \delta) = (\delta - I_{\Theta_0}(\theta))^2$, o estimador de Bayes associado a $\pi$ √© a probabilidade a posteriori $\varphi^{\pi}(x) = \Pr^{\pi}(\theta \in \Theta_0|x)$.

Essa proposi√ß√£o √© consequ√™ncia direta de o estimador de Bayes ser a m√©dia a posteriori de $I_{\Theta_0}(\theta)$.

Consideremos a fam√≠lia exponencial natural
$$
f(x|\theta) = e^{\theta x - \psi(\theta)}.
$$

Para a hip√≥tese nula unilateral $H_0 : \theta \le \theta_0$, um intervalo $[t_1, t_2]$ √© dito conjunto **trunca√ß√£o** para $\varphi$ quando $\varphi(t) = 1$ se $t < t_1$ e $\varphi(t) = 0$ para $t > t_2$. 
No caso de uma hip√≥tese bilateral, vale quando $\varphi(t) = 0$ para $t \not \in [t_1, t_2]$.
Para a hip√≥tese bilateral com $H_0 : \theta \in [\theta_1, \theta_2]$, um estimador $\varphi$ com conjunto trunca√ß√£o $[t_1, t_2]$ √© admiss√≠vel se existe uma medida de probabilidade $\pi_0$ em $[\theta_1, \theta_2]$ e uma medida $\sigma$-finita $\pi_1$ em $[\theta_1, \theta_2]^c$ tal que 
$$
\varphi(x) = \frac{\int f(x|\theta) \pi_0(\theta) \, d\theta}{\int f(x|\theta) \pi_0(\theta) \, d\theta + \int f(x|\theta) \pi_1(\theta) \, d\theta},
$$
para $x \in [t_1, t_2]$. 
Alternativamente, se $\varphi$ √© admiss√≠vel, existem $[t_1, t_2]$, $\pi_0$ e $\pi_1$ satisfazendo a rela√ß√£o acima.

Para o teste $\varphi$ logo acima, quando a distribui√ß√£o amostral √© cont√≠nua com respeito a Lebesgue, o p-valor √© inadmiss√≠vel.

## Regi√µes de confian√ßa

### Intervalos de credibilidade

### Intervalos de confian√ßa cl√°ssicos

### Avalia√ß√£o baseada em teoria da decis√£o

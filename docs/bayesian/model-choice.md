# Escolha de modelos

Podemos considerar a **escolha de modelos** como um caso especial de testagem, afinal estamos testando qual modelo usar.
Todavia, cuidado adicional deve ser tomado porque estamos lidando com modelos potencialmente bastante diferentes, diferente de apenas verificarmos se o par√¢metro de um modelo espec√≠fico mora em uma regi√£o do espa√ßo de par√¢metros.

Estamos agora considerando que a distribui√ß√£o dos dados $f$ √© desconhecida, o que torna mais dif√≠cil condicionar em $x$.
Isso tamb√©m levanta a pergunta se $f$ pertence mesmo √† fam√≠lia considerada $\{f_{\theta} : \theta \in \Theta\}$ e, de forma mais pronfunda, se um modelo verdadeiro de fato existe. 
Considere inicialmente uma situa√ß√£o que temos modelos param√©tricos competindo:
$$
\mathcal{M}_i : x \sim f_i(x|\theta_i). \theta_i \in \Theta_i,  i \in I. 
$$
Do ponto de vista bayesiano, poder√≠amos contruir uma distribui√ß√£o a priori para $I$, e todas as infer√™ncias deveriam ser baseadas na posteriori definida em $I$.
Em geral, $I$ √© um conjunto pequeno, com distribui√ß√µes conhecidas.

---
``üìù`` **Exemplo**

Em problemas de contagem, por exemplo o n√∫mero de acidentes de carro em uma rodovia em um per√≠odo de tempo, estamos modelando $N$. Podemos atribuir duas distribui√ß√µes distitas: $\mathcal{M}_1 : N \sim \operatorname{Poi}(\lambda)$ ou $\mathcal{M}_2 : N \sim \operatorname{NegBin}(m,p)$. 
Note que a dimens√£o dos par√¢metros √© completamente diferente e cada distribui√ß√£o tem suas particularidades.

---

Podemos tamb√©m atribuir modelos n√£o param√©tricos, quando pouca informa√ß√£o sobre o processo gerador √© obtido. 
Nesse caso, $I$ √© infinito e possivelmente n√£o enumer√°vel.
Outro problema que precisamos enfrentar, √© que modelos diferentes podem ter resultados similares e serem apropriados, mesmo que n√£o sejam os verdadeiros (se √© que isso existe!).
Por fim, existe a situa√ß√£o de compararmos modelos em que um √© submodelo do outro. 
Nesse caso, em geral o modelo maior vai apresentar uma perda quadr√°tica menor, por exemplo, mas mais par√¢metros s√£o estimados a partir da mesma amostra. 
O cl√°ssico exemplo dessa situa√ß√£o √© a escolha das vari√°veis que v√£o compor uma regress√£o linear. 

## Framework padr√£o 

### Modelagem a priori

Podemos extender o espa√ßo dos par√¢metros para $\boldsymbol{\Theta} = \cup_{i\in I} \{i\}\times \Theta_i$, em que $\mu \in I$ √© tamb√©m um par√¢metro.
Assim, podemos definir $p_{i}$ como a probabilidade a priori para o modelo $\mathcal{M}_i$. 
Com isso, o Teorema de Bayes diz que 
$$
p(\mathcal{M}_i | x) \propto p_i\int_{\Theta_i}  f_i(x|\theta_i) \pi_i(\theta_i) \, d\theta_i.
$$
Quando $I$ √© infinito, a constru√ß√£o da priori $(\pi_i, p_i)$ para cada $i \in I$ √© delicada. 
Al√©m do mais, quando um modelo $i$ √© submodelo de outro $j$, deveria haver uma coer√™ncia entre $\pi_i$ e $\pi_j$ e, talvez, entre $p_i$ e $p_j$. 
Um outro ponto importante √© que par√¢metros comuns a modelos diferentes devem ser tratados como entidades separadas.
Exce√ß√µes devem ser consideradas caso a caso.

### Fator de Bayes

O fator de Bayes
$$
B_{12} = \frac{P(\mathcal{M}_1|x)}{P(\mathcal{M}_2|x)} \bigg/ \frac{P(\mathcal{M}_1)}{P(\mathcal{M}_2)}
$$
√© usado para comparar os modelos $\mathcal{M}_1$ e $\mathcal{M}_2$. 
O problema acontece quando queremos comparar muitos modelos.

### Crit√©rio de Schwartz

Considere a *expans√£o de Laplace* 
$$
\int_{\Theta} \exp\{n h(\theta)\} \, d\theta = \exp\{n h(\hat\theta)\} (2\pi)^{p/2} n^{-p/2} |H^{-1}(\hat\theta)| + O(n^{-1}),
$$
em que $\hat{\theta}$ √© o argumento m√°ximo de $h$ e $H$√© a Hessiana de $h$. 
Aplicando essa aproxima√ß√£o ao fator de Bayes, obtemos 
$$
B_{12} \approx \frac{L_{1,n}(\hat\theta_{1,n})}{L_{2,n}(\hat\theta_{2,n})}\bigg|\frac{H_1^{-1}(\hat\theta_{1,n})}{H_2^{-1}(\hat\theta_{2,n})}\bigg|\left(\frac{n}{2\pi}\right)^{(p_1-p_2)/2},
$$
em que $L_{i,n}$ √© a verossimilhan√ßa do modelo $i$ para $n$ observa√ß√µes e $\hat{\theta}_{i,n}$ o respectivo argumento m√°ximo.
Portanto, 
$$
\log(B_{12}) \approx \log\lambda_n + \frac{p_2-p_1}{2}\log(n) + K(\hat\theta_{1,2}, \hat\theta_{2,n}),
$$
em que $\lambda_n = L_{1,n}(\hat\theta_{1,n}) / L_{2,n}(\hat\theta_{2,n})$
O crit√©rio de Schwartz √© dado por 
$$
S = -\log \lambda_n - \frac{p_2-p_1}{2}\log(n) 
$$
quando $\mathcal{M}_1 \subset \mathcal{M}_2$ e o termo restante √© negligenci√°vel.

Esse crit√©rio tamb√©m √© conhecido como *Crit√©rio de Informa√ß√£o de Bayes* (BIC). 
Substituindo $1/2$ por $\log(2)$, e temos o primeiro termo multiplicado por 2, temos o *Crit√©rio de Informa√ß√£o Akaike* (AIC). 
Apesar de ser uma aproxima√ß√£o de primeira ordem para o fator de Bayes, a depend√™ncia na priori desaparece, e a compara√ß√£o s√≥ √© v√°lida para modelos regulares, logo a relev√¢ncia em Infer√™ncia Bayesiana √© menor. 

### Desvio bayesiano

Uma alternativa ao AIC e ao BIC √© o *Crit√©rio de Informa√ß√£o de Desvio* (DIC), definido como 
$$
\mathbb{E}[D(\theta)|x] + p_D = \mathbb{E}[D(\theta)|x] + (\mathbb{E}[D(\theta)|x] - D(\mathbb{E}[\theta|x])),
$$
em que $D(\theta) = -2\log(f(x|\theta))$ √© uma medida de desvio. e $p_D$√© uma penaliza√ß√£o. 
Assim, quanto menor o valor de $DIC$, melhor o modelo. 

## Ideias adicionais

> **Modelo m√©dio:** Uma forma de lidar com a escolha de modelos √© n√£o escolher um de fato, mas sim, incluir todos os modelos para lidar com a incerteza do modelo propriamente.
Isso nem sempre √© poss√≠vel em quest√µes cient√≠ficas, j√° que a escolha de um modelo explicativo pode ser relevante.
Al√©m do mais, essa maneira parece infringir a parcim√¥nia.

> **Proje√ß√£o de modelos:** essa abordagem √© baseada na ideia de projetar um modelo $f(y|\theta)$ em submodelos atrav√©s de restri√ß√µes em $\theta$. Isso permite a constru√ß√£o de uma √∫nica priori para $\theta$ e, portanto, acomoda bem prioris impr√≥prias. Dada uma restri√ß√£o $\Theta_0$, uma ideia √© considerar uma restri√ß√£o aceit√°vel se $d(f(\cdot|\theta). \Theta_0)< \epsilon$, em que $d$ √© uma medida de diverg√™ncia, tal como a pseudo-dist√¢ncia de Kullback-Leibler.

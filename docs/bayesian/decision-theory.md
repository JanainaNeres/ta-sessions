# Teoria da Decis√£o

Em geral, a estat√≠stica se preocupa em propor uma decis√£o frente a um problema apresentado. 
Nesse caso, a avalia√ß√£o deve estar clara, como, por exemplo, com a descri√ß√£o do procedimento e suas consequ√™ncias. 
A Teoria da Decis√£o entra para axiomatizar a estrutura de avaliar um estimador para algum par√¢metro. 
O crit√©rio para avaliar uma tomada de decis√£o √© usualmente atrav√©s de uma **fun√ß√£o de perda**. 
Alguns estat√≠sticos discordam de us√°-la, justamente porque defini-la para um problema pode levar a resultados inesperados. 

Seja $\mathcal{D}$ o espa√ßo das decis√µes (por exemplo, uma estimativa √© uma decis√£o) e $\Omega$ o espa√ßo dos par√¢metros.
Uma fun√ß√£o de perda √© uma fun√ß√£o $L : \Omega \times \mathcal{D} \to [0, +\infty]$ e avalia uma penalidade $L(\theta, d)$ em tomar a decis√£o $d$ com respeito a $\theta$. 
Quando $\mathcal{D} = h(\Omega)$, temos que $L(\theta, d)$ mede o erro em obter $h(\theta)$ por $d$.
Escolher uma fun√ß√£o de perda de maneira a considerar o problema em quest√£o n√£o √© uma tarefa f√°cil. 
A complexidade envolvida em defini-la a partir de conceitos subjetivos leva ao uso de perdas matematicamente trat√°veis, como a perda quadr√°tica ou absoluta, por exemplo.

Ent√£o, em uma infer√™ncia bayesiana, do ponto de vista da Teoria da Decis√£o, os tr√™s fatores principais s√£o: a fam√≠lia param√©trica de distribui√ß√µes das observa√ß√µes, a distribui√ß√£o a priori dos par√¢metros, e a fun√ß√£o de perda associada √†s decis√µes. Inclusive a subjetividade em definir a fun√ß√£o de perda e a priori n√£o pode ser separada, conforme destacado por [Lindley (1985)](https://www.wiley.com/en-us/Making+Decisions%2C+2nd+Edition-p-9780471908081). 

---
``üìù`` **Exemplo (Fun√ß√£o de perda)**

Considere o problema de estimar a $\mathbb{E}[x] = \theta$, em que $x \sim Normal(\theta, \sigma^2)$, em que $\sigma^2$ √© conhecido.
Nesse caso, $\mathcal{D} = \Omega = \mathbb{R}$. 
Uma ideia √© usar a perda da forma 
$$L\left(\frac{\delta - \theta}{\sigma}\right), \delta \in \mathcal{D}, \theta \in \Omega,$$
em que o m√≠nimo de $L$ √© em 0. Al√©m disso, a divis√£o pelo desvio padr√£o reduz o vi√©s de vari√¢ncia grande, principalmente quando a dimens√£o de $\theta$ aumenta. 
Usualmente $L(t) = t^2$ √© a perda escolhida.

---

## Fun√ß√£o utilidade

Utilidade √© definida como o oposto de perda e √© utilizada quando se pretende ordenar consequ√™ncias de decis√µes. 
Ou seja, a utilidade sumariza os poss√≠veis resultados de uma decis√£o, como, por exemplo, o lucro da empresa.
Seja $\mathcal{R}$ o espa√ßo das recompensas, que assumimos possuir uma ordena√ß√£o total $\le$ de forma que para todo $r_1, r_2 \in R$, tem-se que $r_1 \le r_2$ ou $r_2 \le r_1$ e $r_1 \le r_2$ com $r_2 \le r_3$ implica $r_1 \le r_3$. 
A primeira propriedade permite comparar qualquer duas recompensas, enquanto a segunda √© a transitividade e fixa uma no√ß√£o que esperar√≠amos de recompensas.

Agora, vamos estender essa no√ß√£o de ordem para $\mathcal{P}$, o espa√ßo das distribui√ß√µes de probabilidade em $\mathcal{R}$.
Assumimos que $\le$ est√° dispon√≠vel em $\mathcal{P}$ e que satisfaz 

(H1) ordem total;

(H2) transitividade.

No caso, de certa forma, $\mathcal{R} \subseteq \mathcal{P}$ atrav√©s das distribui√ß√µes de Dirac com massa em um ponto $r \in \mathcal{R}$ espec√≠fico.

Queremos construir uma fun√ß√£o $U$ em $\mathcal{R}$ que chamaremos de **fun√ß√£o de utilidade** atrav√©s da rela√ß√£o $\le$. 
Atrav√©s da seguinte axiomatiza√ß√£o, conseguimos assegurar tal exist√™ncia.
Observe que se a rela√ß√£o 
$$\mathbb{E}^{P_1}[U(r)] \le \mathbb{E}^{P_2}[U(r)]$$
for equivalente a $P_1 \le P_2$, ent√£o conseguimos determinar a exist√™ncia dessa rela√ß√£o em $\mathcal{P}$, o que d√° uma esp√©cie de rec√≠proca do que queremos encontrar.

Considere o conjunto das distribui√ß√µes definidas em um suporte limitado $\mathcal{P}_{B}$. 
Uma mistura √© definida como $P = \alpha P_1 + (1-\alpha)P_2$, em que $\alpha \in (0,1)$. 
Assumimos que

(H3) Se $P_1 \le P_2$, temos que $\forall P \in \mathcal{P}$, $\alpha P_1 + (1-\alpha) P \le \alpha P_2 + (1-\alpha) P$.

(H4) Se $P_1 \le P_2 \le P_3$, ent√£o existem $\alpha, \beta \in (0,1)$ de forma que, 
$$\alpha P_1 + (1-\alpha) P_3 \le P_2 \le \beta P_1 + (1-\beta)P_3.$$ 

Note que H4 implica que se $r_1 \le r \le r_2$ com $r_1 < r_2$, ent√£o existe um √∫nico $\alpha \in [0,1]$ de forma que $r = \alpha r_1 + (1-\alpha)r_2$. Para demonstrar esse resultado, basta supor a n√£o exist√™ncia e usar um argumento de supremo e √≠nfimo associado √† hip√≥tese de (H4). 
Com esse resultado, dados $r_1 < r_2$, defina $U$ da seguinte forma:
$$
\begin{cases}
    U(r) = v, &\text{se } r_1 \le r \le r_2 \text{ e } r = vr_2 + (1-v)r_1 \\
    U(r) = -\frac{v}{1-v}, &\text{se } r \le r_1 \text{ e } r_1 = vr_2 + (1-v)r \\
    U(r) = \frac{1}{v}, &\text{se } r_2 \le r \text{ e } r_2 = vr + (1-v)r_1.
\end{cases}
$$

Com essa defini√ß√£o, temos que $U(r_1) = 1$ e $U(r_2) = 0$. 
Al√©m do mais, $U$ preserva rela√ß√£o de ordem e se $r = \alpha r_1 + (1-\alpha)r_2$, ent√£o teremos que $U(r) = \alpha U(r_1) + (1-\alpha) U(r_2)$.

Agora, precisamos extender a defini√ß√£o de $U$ para $\mathcal{P}_{B}$, o que existe uma hip√≥tese adicional. 
Defina 
$$
\alpha(r) = \frac{U(r) - U(r_1)}{U(r_2) - U(r_1)}, \beta = \int_{[r_1, r_2]} \alpha(r) dP(r),
$$
em que $P([r_1, r_2]) = 1$. 
Note que $\alpha$ √© obtido a partir das rela√ß√µes do par√°grafo anterior, isto √©, sabemos que para cada $r$, existe $\alpha(r)$, de forma que $r = \alpha(r) r_1 + (1-\alpha(r))r_2$ e obtemos $\alpha$ atrav√©s da f√≥rmula $U(r) = \alpha(r) U(r_1) + (1-\alpha(r)) U(r_2)$.
Al√©m disso $\beta$ indica a probabilidade de selecionarmos $r_1$ quando escolhemos uma loteria $r = \alpha(r) r_1 + (1-\alpha(r))r_2$ segundo a distribui√ß√£o de probabilidade $P$.
Assumimos que 

(H5) P = \beta \delta_{r_2} + (1-\beta) \delta_{r_1}

Com isso, √© poss√≠vel definir a fun√ß√£o de utilidade em $\mathcal{P}_B$. 
Dos resultados que se seguem, considere o seguinte teorema:

> **Teorema:** Sejam $P_1, P_2 \in \mathcal{P}_B$. Ent√£o $P_1 \le P_2$ se, e somente se, $\mathbb{E}^{P_1}[U(r)] \le \mathbb{E}^{P_2}[U(r)]$. Al√©m do mais, se outra fun√ß√£o de utilidade $U^*$ satisfaz a rela√ß√£o de equival√™ncia, ent√£o existem constantes $a > 0$ e $b$ de forma que $U^*(\cdot) = a U(\cdot) + b$.

Com duas hip√≥teses adicionais, podemos extender esse resultado para $\mathcal{P}_E$ que √© o conjunto das distribui√ß√µes $P$ que tem $\mathbb{E}^P[U(r)]$ finita.

Algumas cr√≠ticas ao formalismo incluem: √© imposs√≠vel que um indiv√≠duo consiga comparar quaisquer duas recompensas. 
Al√©m do mais, a transitividade √© algo forte demais. √Äs vezes, resultados da vida real levam √† n√£o transitividade.
A extens√£o de $\mathcal{R}$ para $\mathcal{P}$ tamb√©m √© bastante problematizada, mas explica um pouco da rela√ß√£o da priori com a escolha da fun√ß√£o de perda, no sentido bayesiano.

Um exemplo interessante √© o [paradoxo de Saint Petersburg](https://en.wikipedia.org/wiki/St._Petersburg_paradox) que argumenta que o valor esperado do pr√™mio √© infinito, mas a quantidade que os jogadores recebem √© em geral baixa. 
Uma solu√ß√£o poss√≠vel para esse paradoxo √© mudar a fun√ß√£o de utilidade para uma limitada.

### Rela√ß√£o entre utilidade e perda

A Teoria da Decis√£o assume que cada a√ß√£o $d \in \mathcal{D}$ pode ser avaliada e leva a uma recompensa $r$ com utilidade $U(r)$.
Seja $U(\theta, d) = \mathbb{E}_{\theta, d}[U(r)]$. 
Temos que $U$ mede uma proximidade entre $d$ e $h(\theta)$.
Ap√≥s definir a fun√ß√£o de utilidade, fazemos $L(\theta, d) = -U(\theta, d) \ge 0$ como a fun√ß√£o de perda. 
Note que essa desigualdade implica que $U$ √© limitada superiormente por $0$. 

√â claro que $\min_d L(\theta, d)$, quando $\theta$ √© desconhecido, √© praticamente imposs√≠vel, pois dever√≠amos ter um resultado uniforme em $\Omega$. Por isso, os **frequentistas** usam a no√ß√£o de **perda m√©dia** ou **risco frequentista**:
$$
R(\theta, \delta) = \mathbb{E}_{\theta}[L(\theta, \delta(x))] = \int_{\mathcal{X}} L(\theta, delta(x)) f(x|\theta) \, dx,
$$
em que $\delta(x)$ √© a decis√£o baseada em $x$ quando $x \sim f(x|\theta)$.
Chamamos $\delta$ de **estimador**, enquanto $\delta(x)$ de estimativa. 
No cen√°rio frequentista, estimadores s√£o comparados segundo a performance a longo-prazo, para todos os valores de $\theta$.

Note que $R(\theta, \delta)$ √© uma perda m√©dia ponderada sobre a distribui√ß√£o de  $x$. 
Logo, o dado observado n√£o √© considerado nesse caso, o que √© uma cr√≠tica ao m√©todo.
Al√©m disso, existe uma controv√©rsia sobre a ideia de repetir experimentos, conceito importante para o frequentismo. 
Por fim, para cada $\delta$, temos que $R(\cdot, \delta)$ √© uma fun√ß√£o e, portanto, n√£o induz uma ordem total no conjunto de procedimentos.

No procedimento bayesiano, j√° integramos sobre o espa√ßo de $\Omega$ dos par√¢metros. 
Assim, usamos a **perda esperada a posteriori**:
$$
\varrho(\pi, d \mid x) = \mathbb{E}^{\pi}[L(\theta, d)| x] = \int_{\Omega} L(\theta, d) \pi(\theta \mid x) \, d \theta, 
$$
em que $\pi(\theta \mid x) \propto f(x \mid \theta)\pi(\theta)$. 
O **erro integrado** √© definido como 
$$
r(\pi, \delta) = \mathbb{E}^{\pi}[R(\theta, \delta)] = \int_{\Omega} \int_{\mathcal{X}} L(\theta, \delta(x)) f(x|\theta) \, dx \, \pi(\theta) \, d\theta = \int_{\mathcal{X}} \varrho(\pi, \delta(x) | x) m(x) \, dx,
$$
em que a √∫ltima rela√ß√£o √© uma aplica√ß√£o do Teorema de Fubini dado que $L \ge 0$.
Al√©m do mais, para minimizar $r(\pi, \delta)$, para cada $x$, podemos tomar $d = \delta(x)$ que minimiza $\varrho(\pi, d | x)$, pela √∫ltima igualdade da express√£o acima.

> **Estimador de Bayes:** Seja uma priori $\pi$ e uma perda $L$. 
O estimador de Bayes √© $\delta^{\pi}$ que minimiza $r(\pi, \delta)$. 
Em particular, para cada $x$, temos que $\delta^{\pi}(x) = \arg \min_d \varrho(\pi, d | x)$. 
O risco bayesiano √© o valor $r(\pi) = r(\pi, \delta^{\pi})$.

Note que para perdas estritamente convexas, o estimador de Bayes √© √∫nico.

## Maximalidade e admissibilidade

Considere $\mathcal{D}^*$ o espa√ßo das distribui√ß√µes de probabilidade em $\mathcal{D}$. 
Um **estimador aleatorizado** $\delta^*$ significa tomar uma decis√£o de acordo com a densidade de probabilidade $\delta^*(x, \cdot)$. 
A perda √© definida como 
$$
L(\theta, \delta^*(x)) = \int_{\mathcal{D}} L(\theta, a) \delta^*(x, a) \, da.
$$
Usar esse estimador n√£o √© usual porque ele adiciona ru√≠do em um fen√¥meno para tomar uma decis√£o sob incerteza.
Al√©m do mais, ele n√£o obedece o Princ√≠pio da Verossimilhan√ßa, dado que para o mesmo valor de $x$, podem existir v√°rios valores estimados.

---
``üìù`` **Exemplo (Estimador randomizado)**

Podemos definir um estimador randomizado segundo
$$
\delta^*(x_1, x_2)(t) = \begin{cases}
    1_{2t = x_1 + x_2} &\text{se } x_1 \neq x_2  \\
    [1_{t = x_1 - 1} + 1_{t = x_1 + 1}]/2 &\text{se } x_1 = x_2,
\end{cases}
$$
em que $1_{v}$ √© a massa de Dirac em $v$. 

---

Para toda priori $\pi$, o risco de Bayes √© o mesmo no conjunto dos estimadores randomizados e n√£o randomizados, isto √©, 
$$
\inf_{\delta \in \mathcal{D}} r(\pi, \delta) = \inf_{\delta^* \in \mathcal{D}^*} r(\pi, \delta^*) = r(\pi).
$$
Como um procedimento randomizado √© a m√©dia de riscos de estimadores n√£o randomizados, ele n√£o pode melhor√°-los.

### Maximalidade

> **Risco minimax**: $\bar{R} = \inf_{\delta \in \mathcal{D}^*} \sup_{\theta} R(\theta, \delta) = \inf_{\delta \in \mathcal{D}^*} \sup_{\theta} \mathbb{E}_{\theta}[L(\theta, \delta(x))]$. 
Um estimador minimax √© um estimador $\delta_0$ que satisfaz $\sup_{\theta} R(\theta, \delta_0) = \bar{R}$. 

Note que esse estimador, toma o pior caso para $\theta$ e ent√£o minimiza para os procedimentos desse pior caso. 
Esse m√©todo enxerga a natureza como um agente inimigo que tende a escolher o pior caso.

O estimador minimax nem sempre existe. 
Para isso, condi√ß√µes suficientes precisam ser estudadas.
Se $\Omega$ √© finito e $L$ √© cont√≠nua, ent√£o existe uma estrat√©gia minimax.
Outra proposta √© verificar que o conjunto das fun√ß√µes de risco em $\mathcal{D}$ √© compacto em um espa√ßo maior em que $\mathcal{D}$ est√° inserido e que a perda √© constante.

**Teorema:** Se $\mathcal{D} \subseteq \mathbb{R}^k$ √© um conjunto convexo compacto e $L(\theta, d)$ √© cont√≠nua e convexa como fun√ß√£o de $d$ para $\theta$ fixado, ent√£o existe um estimador minimax n√£o randomizado. 
O estimador ser√° n√£o randomizado pela [desigualdade de Jensen](https://en.wikipedia.org/wiki/Jensen%27s_inequality). 
Esse resultado √© um caso particular do *Teorema Rao-Blackwell*.

O *risco de Bayes* √© sempre menor do que o *risco minimax*, o que √© expresso matematicamente por 
$$
\underbar{R} = \sup_{\pi} r(\pi) = \sup_{\pi} \inf_{\delta \in \mathcal{D}} r(\pi, \delta) \le \bar{R} = \inf_{d \in \mathcal{D}^*} \sup_{\theta} R(\theta, \delta).
$$
A **distribui√ß√£o menos favor√°vel** √© $\pi^*$ tal que $r(\pi^*) = \underbar{R}$. 
O problema de estima√ß√£o tem um *valor* quando $\underbar{R} = \bar{R}$.

Um resultado interessante √© que se $\delta$ √© estimador de Bayes com respeito a $\pi$ e $R(\theta, \delta) \le r(\pi)$ para todo $\theta \in \Omega$, ent√£o $\delta$ √© estimador minimax e $\pi$ √© a distribui√ß√£o menos favor√°vel.

> **Teorema:** Considere um problema estat√≠stico que possua um valor, uma distribui√ß√£o menos favor√°vel $\pi_0$ e um estimador minimax $\delta^{\pi_0}$. 
Ent√£o se $\Omega \subseteq \mathbb{R}$ √© compacto e $R(\theta, \delta^{\pi_0})$ √© fun√ß√£o anal√≠tica de $\theta$, ent√£o $\pi_0$ tem suporte finito ou $R(\theta, \delta^{\pi_0})$ √© constante.

Esse teorema mostra que o minimax n√£o √© um bom estimador do ponto de vista bayesiano, dado que (1) ele pode ser randomizado ou (2) ele pode levar a prioris n√£o real√≠sticas com suporte finito.

### Admissibilidade

> **Admissibilidade:** Um estimador $\delta_0$ √© inadmiss√≠vel se existe um estimador $\delta_1$ que domina $\delta_0$, isto √©, $R(\theta, \delta_0) \ge R(\theta, \delta_1)$ para todo $\theta \in \Omega$, e pelo menos para um valor $\theta_0$, vale a desigualdade estrita. 
Caso contr√°rio, o estimador √© admiss√≠vel.

Construir um estimador apenas considerando a admissibilidade n√£o √© uma boa estrat√©gia, afinal $\delta(x) = \theta_0 \in \Omega$ √© um estimador que tem valor exato para $\theta = \theta_0$.
Logo, faz sentido considerar maximalidade simultaneamente. 
O interessante √© que se existe um √∫nico estimador minimax, ent√£o ele √© admiss√≠vel. 
A rec√≠proca √© falsa em geral, mas se $\delta_0$ √© admiss√≠vel com risco constante, ent√£o ele √© o √∫nico estimador minimax.

A rela√ß√£o de admissibilidade com estimadores de Bayes √© bem estrita: 

(1) Se a priori $\pi$ √© estritamente positiva em $\Omega$, com risco de Bayes finito, e a fun√ß√£o de risco √© cont√≠nua em $\theta$ para todo $\delta$, ent√£o o estimador de Bayes $\delta^{\pi}$ √© admiss√≠vel.

(2) Se o estimador de Bayes $\delta^{\pi}$ √© √∫nico, ent√£o ele √© admiss√≠vel.

## Perdas cl√°ssicas

Essas perdas s√£o trat√°veis matematicamente e bem documentadas, mesmo que n√£o representem perfeitamente o problema em quest√£o. 

### Perda quadr√°tica

√â definida como $L(\theta, d) = (\theta - d)^2$. 
Provavelmente a perda mais utilizada.
Penalizada fortemente desvios altos.
Mas, como a perda √© convexa e vale a desigualdade de Jensen mencionada mais acima, o que exclui estimadores randomizados.
O interessante √© que, sob essa perda, o estimador de Bayes √© a m√©dia a posteriori, um dos valores que pensar√≠amos naturalmente, mesmo sem adicionar a carga da teoria da decis√£o.

**Proposi√ß√£o:** O estimador de Bayes $\delta^{\pi}$ associado com a perda quadr√°tica $L$ √© a esperan√ßa a posteriori $\delta^{\pi}(x) = \mathbb{E}^{\pi}[\theta | x]$. 
O resultado imediato ocorre quando $L(\theta, \delta) = w(\theta)(\theta - \delta)^2$, como uma pondera√ß√£o. 
Nesse caso, o estimador de Bayes √© 
$$\delta^{\pi}(x) = \frac{\mathbb{E}^{\pi}[w(\theta) \theta | x]}{\mathbb{E}^{\pi}[w(\theta)| x]}.$$

### Perda absoluta

Uma alternativa √† perda quadr√°tica √© $L(\theta, d) = |\theta - d|$, que pode ser generalizada para 
$$
L_{k_1, k_2}(\theta, \delta) = \begin{cases}
k_2(\theta - d) &\text{se } \theta > d \\
k_1(d - \theta) &\text{c.c.}
\end{cases}
$$
A penaliza√ß√£o para desvios maiores √© menor, apesar de manter a convexidade.
√© poss√≠vel tamb√©m propor uma perda como uma mistura dessas perdas. 
Em uma regi√£o pr√≥xima de zero, usamos a perda quadr√°tica. Depois, usamos a perda absoluta.
Com essa perda, por exemplo, n√£o existe estimador de Bayes em forma fechada.

O estimador de Bayes associado a $L_{k_1, k_2}(\theta, \delta)$ e a $\pi$ √© um quartil $k_2/(k_1 + k_2)$ de $\pi(\theta | x)$. Em particular, quando $k_1 = k_2$, o estimador √© a mediana a posteriori.

### Perda 0-1

Essa perda √© mais utilizada no contexto de teste de hip√≥teses. Ela √© definida como $L(\theta, \delta) = 1 - 1_{\theta = \delta}$.

---
``üìù`` **Exemplo (Teste de hip√≥teses)**

Seja o teste de hip√≥teses $H_0 : \theta \in \Omega_0$ e $H_1 : \theta \in \Omega_1$. 
Ent√£o $\mathcal{D} = \{0, 1\}$ em que $0$ significa rejeitar $H_0$.
Logo queremos estimar a fun√ß√£o $1_{\theta \in \Omega_0}$. 
O risco frequentista √© 
$$
R(\theta, \delta) = \begin{cases}
    \Pr_{\theta}(\delta(x) = 0), &\text{se } \theta \in \Omega_0 \\
    \Pr_{\theta}(\delta(x) = 1), &\text{c.c.,}
\end{cases}
$$
que s√£o os erros do tipo 1 e do tipo 2, respectivamente.

---

O estimador de Bayes √© dado por 
$$
\delta^{\pi}(x) = \begin{cases}
    1 &\text{se }\Pr(\theta \in \Omega_0 | x) > \Pr(\theta \in \Omega_1 | x) \\
    0, &\text{c.c.}
\end{cases}
$$

### Perdas intr√≠nsecas

√Äs vezes, estamos em uma situa√ß√£o n√£o informativa sobre a parametriza√ß√£o natural e a escolha da fun√ß√£o de perda.
O estimador de Bayes n√£o √© invariante por transforma√ß√µes biun√≠vocas em geral.
Dessa forma, pode ser interessante obter perdas invariantes.
Nesse caso, comparar $f(\cdot | \delta)$ com $f(\cdot | \theta)$ pode ser interessante, isto √©, definir 
$$
L(\theta, \delta) = d(f(\cdot | \theta), f(\cdot | \delta)).
$$
Duas dist√¢ncias usuais s√£o: (1) entropia, Kullback‚ÄìLeibler divergence, ou (2) Hellinger. Elas resultam nas seguintes perdas:

$$
(1) L_e(\theta, \delta) = \mathbb{E}_{\theta}\left[\log\left(\frac{f(x|\theta)}{f(x|\delta)}\right)\right].
$$

$$
(2) L_H(\theta, \delta) = \frac{1}{2}\mathbb{E}_{\theta}\left[\left(\sqrt{\frac{f(x|\delta)}{f(x|\theta)}} - 1\right)^2\right].
$$

## Links

- [Optimal Statistical Decisions, Morris DeGroot](https://onlinelibrary.wiley.com/doi/book/10.1002/0471729000): esse livro expande os resultados de teoria da decis√£o apresentados no livro do Robert.
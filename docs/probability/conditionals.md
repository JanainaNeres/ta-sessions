# Distribui√ß√µes e Esperan√ßas condicionais

Vamos iniciar com o caso em que $X$ e $Y$ s√£o discretas.
Considere um espa√ßo de probabilidade $(\Omega, \mathcal{A}, P)$ e $A \in \mathcal{A}$ tal que $P(A) > 0$.
A **distribui√ß√£o conditional de $X$ dado o evento $A$** √©
$$
P(X \in B | A) = \frac{P(X \in B, A)}{P(A)},
$$
em que $B$ √© um boreliano da reta real.
Podemos verificar que $P(X|A)$ define uma probabilidade, visto que satisfaz os tr√™s axiomas da probabilidade: probabilidade n√£o negativa,  probabilidade de $X \in \R | A$ sendo $1$ e probabilidade da uni√£o sendo a soma das probabilidade individuais para sequ√™ncias de disjuntos.

A **fun√ß√£o de distribui√ß√£o condicional de $X$ dado $A$** √© 
$$
F_X(x|A) = P(X \le x|A).
$$
e a **esperan√ßa conditional** √©
$$
E(X|A) = \int x dF_X(x|A).
$$

Agora suponha que $Y$ seja uma vari√°vel discreta em $(\Omega, \mathcal{A}, P)$ que assuma os valores $y_1, y_2, \dots$.
Ent√£o, podemos usar os conjuntos $A_n = \{Y = y_n\}$ para particionar $\Omega$.
Nesse caso, 
$$
P(X \in B | Y = y_n) = P(X \in B| A_n)
$$
√© a **distribui√ß√£o condicional de $X$ dado $Y=y_n$**.

Da√≠ podemos calcular 
$$
E[X] = \sum_{n=1} ^{\infty} E[X|Y=y_n] P(Y=y_n).
$$
A **esperan√ßa condicional de $X$ dado $Y$** √© $E[X|Y] = E[X|Y=y]$, que √© uma fun√ß√£o de $y$.
Em particular, $E[X|Y]$ √© uma vari√°vel aleat√≥ria cujas realiza√ß√µes s√£o $E[X|Y=y]$.
Portanto, 
$$
E[X] = E[E[X|Y]],
$$
usando que 
$$E[E[X|Y]] = \int E[X|Y=y] dF_Y(y) = E[X].$$

---
``üìù`` **Processo de Poisson**

Considere um contador de part√≠culas que conta segundo um processo de Poisson com par√¢metro $\lambda > 0$.
O n√∫mero de part√≠culas contadas at√© o tempo $t > 0$ √© $X_t \sim Poisson(\lambda t)$
Seja $T_1$ o tempo de chegada da primeira part√≠cula e $T_n$ o tempo entre as chegadas das part√≠culas $n-1$ e $n$.
Vimos que $T_1, T_2, \dots$ s√£o independentes e identicamente distribu√≠dos com par√¢metro $\lambda$.

Uma pergunta natural √© qual a distribui√ß√£o de $T_1 | X_t = 1$? A resposta √© uniforme $[0,t]$, isto √©, se sabemos que chegou exatamente um indiv√≠duo no per√≠odo $[0,t]$, o tempo que ele chegou est√° uniformemente distribu√≠do em $[0,t]$.
Note que $X_t = 1$ implica que $T_1 \le t$.
Dessa forma,
$$
P(T_1 \le s | X_t = 1) = \begin{cases}
 0, &s \le 0 \\
 1, &s > t
 \end{cases}.
$$
Se $0 < s < t$,
$$
\begin{split}
P(T_1 \le s | X_t = 1) &= \frac{P(T_1 \le s, X_t = 1)}{P(X_t = 1)} \\
&= \frac{P(X_s = 1, X_t - X_s = 0)}{P(X_t = 1)} \\
&= \frac{P(X_s = 1)P(X_t - X_s = 0)}{P(X_t = 1)} \\
&=\frac{\lambda s e^{-\lambda s} e^{-\lambda(t-s)}}{\lambda t e^{-\lambda t}} = \frac{s}{t}.
\end{split}
$$
em que a segunda igualdade vem do fato que $T_1 \le s$ e $X_t = 1$ implica $X_s = 1$ e, portanto, $X_t - X_s = 0$.
A segunda igualdade vem do fato que os intervalos disjuntos s√£o independentes no processo de Poisson.
Finalmente, a terceira vem do fato que as vari√°veis t√™m distribui√ß√£o de Poisson e $X_t - X_s$ tem a mesma distribui√ß√£o de $X_{t-s} - X_0 = X_{t-s}$.
Portanto $T_1 | X_t = 1 \sim U[0,t]$.

Mais do que isso, podemos provar que 
$$
(T_1, T_1 + T_2, \dots, T_1 + \dots + T_n)
$$
tem distribui√ß√£o uniforme no conjunto
$$
A_n = \{(x_1, \dots, x_n) \in \mathbb{R}^n : 0 \le x_1 \le x_2 \le \dots \le x_n \le t\}.
$$
Note que se $Y_1, \dots, Y_n$ s√£o iid $U[0,t]$, a estat√≠stica de ordem $Y_{(1)}, \dots, Y_{(n)}$ tem distribui√ß√£o uniforme em $A_n$.

Com isso em mente, podemos pensar que dado que $n$ part√≠culas chegaram at√© o tempo $t$, isto √©, $X_t = n$, a chegada das $n$ part√≠culas √© modelada por $n$ vari√°veis aleat√≥rias iid $U[0,t]$ e, ap√≥s ordenadas, temos os tempos de chegadas das part√≠culas em ordem.

---

## Caso geral

Seja $I$ um intervalo de tamanho $\Delta y$ e que contenha o ponto $y$.
Assim, dizemos que 
$$
P(X \in B|Y=y) = \lim_{\Delta y \to 0} P(X \in B| Y \in I),
$$
se o limite existe. 
Se para alguma vizinhan√ßa de $y$, $P(Y \in I) = 0$, ent√£o definimos $P(X \in B | Y=y) = P(X \in B)$.
Essa defini√ß√£o de probabilidade condicional n√£o √© √∫nica.
Al√©m dela, existem baseadas em Teoria da Medida que d√£o um rigor maior em demostra√ß√µes te√≥ricas.

Podemos provar tamb√©m que a **distribui√ß√£o condicional de $X$ dado $Y$** √© a √∫nica que satisfaz
$$
F_{X,Y}(x,y) = \int_{-\infty}^y F_X(x|Y=t) dF_Y(t), (x,y) \in \mathbb{R}^2
$$
para quaisquer pares de vari√°veis aleat√≥rias $X$ e $Y$ definidas em um mesmo espa√ßo de probabilidade.

Suponha que $X$ e $Y$ tenham densidade $f(x,y)$.
Seja 
$$
f(x|y) = \frac{f(x,y)}{\int f(x,y) \, dy} = \frac{f(x,y)}{f_Y(y)}
$$
a **densidade da distribui√ß√£o condicional**.
Se $f_Y(y) > 0$, ent√£o $f(x|y)$ define uma densidade.
Al√©m disso, essa densidade satisfaz as rela√ß√µes que estabelecemos para probabilidade condicional.
Se $f_Y(y) = 0$, os valores para a densidade n√£o s√£o relevantes, pois tem probabilidade $0$.

Formalmente, definimos $P(X \in B|Y=y)$ como a **distribui√ß√£o condicional para $X$ dado $Y$** se 

**(I)** $\forall y \in \mathbb{R}$, $P(X \in B | Y=y)$ define uma probabilidade nos borelianos da reta.

**(II)** Para todo boreliano $B$, $P(X \in B | Y=y)$ √© fun√ß√£o mensur√°vel de $y$ e para todo $(x,y) \in \mathbb{R}^2$,
$$
\int_{-\infty}^y P(X \le x | Y=t) dF_Y(t) = P(X\le x, Y \le y)
$$

Pode-se **provar** que para duas vari√°veis aleat√≥rias $X$ e $Y$ definidas no mesmo espa√ßo de probabilidade, existe uma distribui√ß√£o condicional para $X$ dado $Y$, e ela √© √∫nica exceto em um conjunto de medida nula.

Apesar de 
$$
P(X \in B|Y=y) = \lim_{\Delta y \to 0} P(X \in B| Y \in I),
$$
ser uma verdade, esse limite acerta com probabilidade $1$, mas n√£o em todo ponto. 
Para distribui√ß√µes cont√≠nuas, podemos obter resultados inesperados.
Em geral, o que se faz √© usar a densidade conjunta para vari√°veis cont√≠nuas ou avaliar no caso discreto. 
Se n√£o conseguirmos proceder com esses casos, temos que chutar a distribui√ß√£o condicional e verificar as condi√ß√µes da defini√ß√£o.

Uma proposi√ß√£o importante √© a seguinte, tamb√©m conhecido como **princ√≠pio da substitui√ß√£o**.

**Proposi√ß√£o:** Sejam $X$ e $Y$ vari√°veis aleat√≥rias e $\phi(x,y)$ uma fun√ß√£o mensur√°vel.
Se a distribui√ß√£o condicional de $X$ dado $Y$ √© 
$$
P(X \in B | Y=y), B \in \mathcal{B}, y \in \mathbb{R},
$$
ent√£o
$$
P(\phi(X,Y) \in B|Y=y) = P(\phi(X,y) \in B | Y=y) = P( X \in \{x : \phi(x,y) \in B\}|Y=y).
$$

## Esperan√ßa condicional

Sejam $X$ e $Y$ vari√°veis aleat√≥rias.
A esperan√ßa condicional de $X$ dado $Y=y$ √© 
$$
E[X|Y=y] = \int x \, dF_X(x|Y=y).
$$
Se $X$ √© integr√°vel, ent√£o essa esperan√ßa existe e √© finita quase certamente para valores de $y$.
A vari√°vel aleat√≥ria $E[X|Y]$, que √© fun√ß√£o de $Y$, √© a **esperan√ßa condicional de $X$ dado $Y$**.

Lembrando que $E[X] = E[E[X|Y]]$.
Al√©m disso, se $X_1 \le X_2$, vale que $E[X_1 | Y] \le E[X_2 \le Y]$.
A linearidade e a desigualdade de Jensen do valor esperado tamb√©m √© mantida.
Outros resultados que valem para valor esperado $E[\cdot]$ tamb√©m valem para $E[\cdot | Y]$.
# Estima√ß√£o pontual

Um **estimator pontual** de uma $g(\theta)$ √© uma estat√≠stica que toma valores em $\operatorname{Imagem}(g)$.

## Estimador n√£o enviesado

Mais formalmente:

> Sejam $\Omega$ espa√ßo de par√¢metros de uma fam√≠lia param√©trica de distribui√ß√µes $P_{\theta}$ e $g : \Omega \to G$ uma fun√ß√£o mensur√°vel (cont√≠nua, por exemplo). 
Uma fun√ß√£o $\phi : \mathcal{X} \to G' \supseteq G$ √© **estimador** de $g(\theta)$, em que $\mathcal{X}$ √© o espa√ßo amostral.
Dizemos que $\phi$ √© **n√£o enviesado** se 
> $$
\mathbb{E}_{\theta}[\phi(X)] = g(\theta), \forall \theta \in \Omega.
> $$
> Al√©m do mais, o vi√©s de um estimador √© dado por $b_{\phi}(\theta) = \mathbb{E}_{\theta}[\phi(X)] - g(\theta)$.

Um estimador √© uma fun√ß√£o das amostras, enquanto uma **estimativa** √© uma avalia√ß√£o dessa fun√ß√£o em observa√ß√µes.

---
``üìù`` **Exemplo (Normal)**

Seja $X_1 \dots, X_n \overset{iid}{\sim} Normal(\mu, \sigma^2)$, em que $\theta = (\mu, \sigma^2) \in \mathbb{R} \times \mathbb{R}_+$ √© o par√¢metro.
Suponha que estamos interessados em $g(\theta) = \mu$.
Um estimador para $\mu$ √© $\phi(X_1, \dots, X_n) = X_1$ ou $\phi(X_1, \dots, X_n) = \min\{X_i\}$. 
Todavia, um estimador com melhores propriedades, que veremos ao decorrer do curso, √© $\phi(X_1, \dots, X_n) = \bar{X}$. 
Note que 
$$
\mathbb{E}_{\mu}[\bar{X}] = \frac{1}{n}\sum_{i=1}^n \mathbb{E}_{\mu}[X_i] = \mu,
$$
o que mostra que $\bar{X}$ √© n√£o enviesado.

---

Aparentemente, um estimador ser n√£o enviesado parece muito bom, n√£o √©? Mas nem sempre isso √© suficiente ou poss√≠vel. 
No exemplo a seguir, mostramos que n√£o existe estimador n√£o enviesado para a taxa da distribui√ß√£o exponencial.

---
``üìù`` **Exemplo (Exponencial)**

Seja $X \sim Exponencial(\lambda)$, em que $\lambda \in \R_+$ √© o par√¢metro. 
Seja $\phi(X)$ um estimador n√£o enviesado para $\lambda$.
Ent√£o
$$
\mathbb{E}_{\lambda}[\phi(X)] = \lambda \int_0^{+\infty} \phi(x) \exp(-\lambda x) \, dx = \lambda, \forall \lambda \in \R_+. 
$$
Dividindo ambos os lados por $\lambda$ e diferenciando com respeito a $\lambda$, pela [Regra de Leibniz](https://en.wikipedia.org/wiki/Leibniz_integral_rule):
$$
- \int_0^{+\infty} x\phi(x) \exp(-\lambda x) \, dx = 0.
$$
Mas $x\exp(-\lambda x) > 0, \forall x > 0$. 
Se $\phi(x) > 0$ em um conjunto de medida positiva, pelo que estudamos em [Integra√ß√£o](https://lucasmoschen.github.io/ta-sessions/infestatistica_MSc/probability/#integracao), ter√≠amos que a integral deveria ser positiva, o que √© um absurdo. 
Com isso $\phi(x) = 0$ quase certamente e, portanto, n√£o √© um estimador n√£o enviesado. 
Conclu√≠mos que n√£o pode haver um estimador n√£o enviesado para $\lambda$.

---

Um estimador ser n√£o enviesado implique que, na pr√°tica, se fossem feitos infinitos experimentos e calcul√°ssemos o valor do estimador, o valor m√©dio convergiria para o valor verdadeiro do par√¢metro. 
Isso ocorre pela Lei dos Grandes N√∫meros.
Mas na pr√°tica, fazemos uma quantidade finita de experimentos e isso pode impactar nossos resultados. 
No pr√≥ximo exemplo, vamos mostras que um estimador n√£o enviesado e vamos visualizar sua distribui√ß√£o.

---
``üìù`` **Exemplo (Correla√ß√£o normal multivariada)**

Seja $(X_1, Y_1), \dots, (X_n,Y_n) \sim Normal(\boldsymbol{\mu}, \Sigma)$, com m√©dia $\mu = (0,0)$ e matriz de correla√ß√£o $\Sigma = [[1, \rho], [\rho, 1]]$. 
Assim o par√¢metro de interesse √© a correla√ß√£o entre $X$ e $Y$ $\rho \in [-1,1]$.
Considere o estimador para $\rho$ para $n$ amostras sendo
$$
\phi(X_1, \dots, X_n) = \dfrac{\sum_{i=1}^n (X_i - \bar{X})(Y_i - \bar{Y})}{\sqrt{\sum_{i=1}^n (X_i - \bar{X})^2}\sqrt{\sum_{i=1}^n (Y_i - \bar{Y})^2}},
$$
conhecido como **correla√ß√£o emp√≠rica** ou **correla√ß√£o de Pearson**.
Apesar de famoso, esse estimador tem vi√©s n√£o nulo. 
Todavia, pode-se corrigi-lo para obtermos um estimador n√£o enviesado [(Olkin and Pratt, 1958)](https://projecteuclid.org/journals/annals-of-mathematical-statistics/volume-29/issue-1/Unbiased-Estimation-of-Certain-Correlation-Coefficients/10.1214/aoms/1177706717.full),
da forma
$$
\psi = G(\phi) = \phi F\left(\frac{1}{2}, \frac{1}{2}; (n-1)/2; 1 - \phi^2\right),
$$
que tem a propriedade de n√£o ser enviesado, em que $F$ √© a fun√ß√£o hipergeom√©trica de Gauss.
Como $\phi$ e $\psi$ s√£o fun√ß√µes mensur√°veis das amostras, eles tamb√©m t√™m uma distribui√ß√£o amostral.
Uma maneira de visualizar essas distribui√ß√µes √© atrav√©s de simula√ß√µes de Monte Carlo.

1) Geramos $n$ amostras $(X_i, Y_i)$ da normal multivariada.
2) Calculamos $\phi$ e $\psi$. 
3) Repetimos esse processo $M$ vezes e temos amostras da distribui√ß√£o de $\phi$ e $\psi$.

```python
# Par√¢metros
rho = 0.1
n = 30
M = 100000
phi_samples = np.zeros(M)
psi_samples = np.zeros(M)
# Gerando os dados de acordo com a distribui√ß√£o especificada
rng = np.random.RandomState(1001)
for i in range(M):
    Z = rng.multivariate_normal(mean=[0,0], cov=[[1,rho], [rho,1]], size=n)
    X = Z[:,0]
    Y = Z[:,1]
    # Calculando ~ correla√ß√£o de Pearson
    phi_samples[i] = np.corrcoef(X,Y)[0,1]
    psi_samples[i] = phi_samples[i] * hyp2f1(1/2, 1/2, (n-1)/2, 1 - phi_samples[i]**2)

# Desenhando as distribui√ß√µes
plt.hist(phi_samples, label=r'$\phi$', color='pink', bins=50)
plt.hist(psi_samples, label=r'$\psi$', color='blue', bins=50)
plt.legend()
plt.show()
```

![png](output_3_0.png)
    

Nesse caso, temos que o Erro Absoluto Percentual M√©dio do estimador $\psi$ √©, aproximadamente,  
$$
\mathbb{E}_{\rho}\left[\bigg|\frac{\psi - \rho}{\rho}\bigg|\right] \approx 150\%.
$$
Ou seja, apesar de $\psi$ ser n√£o enviesado e $\phi$ ter vi√©s baixo, a vari√¢ncia dos estimadores √© bem grande. 
Nesse caso, temos uma probabilidade alta de obter uma estima√ß√£o 100% maior ou menor do que o valor verdadeiro de $\rho$.

Note que a diferen√ßa entre os estimadores √© pequena, pois $n$ √© razoavelmente grande e $\psi$ tem vi√©s baixo.

---

### Erro quadr√°tico

Seja $L(\theta, d) = (\theta - d)^2$ a perda quadr√°tica.
A fun√ß√£o de risco $R(\theta, \phi)$ para o estimador $\phi(X)$ √© dada por 
$$
\begin{split}
R(\theta, \phi) &= \mathbb{E}_{\theta}[(\theta-\varphi(X))^2] \\
&= \mathbb{E}_{\theta}[(\theta-\mathbb{E}[\varphi(X)] + \mathbb{E}[\varphi(X)] - \varphi(X))^2] \\
&= (\theta-\mathbb{E}[\varphi(X)])^2 + 2(\theta-\mathbb{E}[\varphi(X)])(\mathbb{E}_{\theta}[\mathbb{E}[\varphi(X)] - \varphi(X))] + \mathbb{E}_{\theta}[(\mathbb{E}[\varphi(X)] - \varphi(X))^2] \\ 
&= b_{\phi}(\theta)^2 + 2(\theta-\mathbb{E}[\varphi(X)])(\mathbb{E}[\varphi(X)]-\mathbb{E}[\varphi(X)]) + \operatorname{Var}(\phi(X)) \\
&= b_{\phi}(\theta)^2 + \operatorname{Var}(\phi(X)).
\end{split}
$$
Portanto, para estimadores n√£o enviesados, o risco √© dado somente pela vari√¢ncia do estimador.

## Estima√ß√£o n√£o enviesada de menor vari√¢ncia

> Um estimador n√£o enviesado $\phi$ √© **uniformly minimum variance unbiased estimator (UMVUE)** (estimador n√£o enviesada de vari√¢ncia uniformemente m√≠nima) se $\phi$ tem vari√¢ncia finita e, para todo estimador n√£o enviesado, $Var_{\theta} \phi(X) \le Var_{\theta} \psi(X), \forall \theta \in \Omega$.

Essa defini√ß√£o leva ao seguinte importante teorema:

**Teorema (Lehmann-Scheff√©):** Se $T$ √© uma estat√≠stica completa, ent√£o todos os estimadores n√£o enviesados de $g(\theta)$, que s√£o fun√ß√µes de $T$, mas n√£o de $X$, s√£o iguais quase certamente para todo $\theta$.
Al√©m disso, se existe um estimador n√£o enviesado que √© fun√ß√£o de uma estat√≠stica suficiente completa, ent√£o ele √© UMVUE.

Cap 5.1.1 Schervish (298 - 301)
Cap 7.3.2 Casella (330 - 342)
Cap 5.1 Keenet (61 - 66)

## Limites inferiores para a vari√¢ncia

Cap 5.1.2 Schervish (301  307)
Cap 4.5 Keener (71 - 77)

## Estimador de M√°xima Verossimilhan√ßa

Cap 5.1.3 Schervish (307 - 309)
Cap 7.2.2 Casella (315 - 324)

Obs.: incluir alguns pensamentos de cap 7.3.3 Cassella (342 - 348)

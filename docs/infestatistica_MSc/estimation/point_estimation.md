# Estima√ß√£o pontual

Um **estimator pontual** de uma $g(\theta)$ √© uma estat√≠stica que toma valores em $\operatorname{Imagem}(g)$.
Mais formalmente:

> Sejam $\Omega$ espa√ßo de par√¢metros de uma fam√≠lia param√©trica de distribui√ß√µes $P_{\theta}$ e $g : \Omega \to G$ uma fun√ß√£o mensur√°vel (cont√≠nua, por exemplo). 
Uma fun√ß√£o $\phi : \mathcal{X} \to G' \supseteq G$ √© **estimador** de $g(\theta)$, em que $\mathcal{X}$ √© o espa√ßo amostral.

Vamos explorar formas de comparar estimadores pontuais.

## Vi√©s de um estimador

> Dizemos que $\phi$ √© **n√£o enviesado** se 
> $$
\mathbb{E}_{\theta}[\phi(X)] = g(\theta), \forall \theta \in \Omega.
> $$
> Al√©m do mais, o vi√©s de um estimador √© dado por $b_{\phi}(\theta) = \mathbb{E}_{\theta}[\phi(X)] - g(\theta)$.
> Se existe um estimador n√£o enviesado para $g(\theta)$, dizemos que $g$ √© U-estim√°vel.

Um estimador √© uma fun√ß√£o das amostras, enquanto uma **estimativa** √© uma avalia√ß√£o dessa fun√ß√£o em observa√ß√µes.

---
``üìù`` **Exemplo (Normal)**

Seja $X_1 \dots, X_n \overset{iid}{\sim} Normal(\mu, \sigma^2)$, em que $\theta = (\mu, \sigma^2) \in \mathbb{R} \times \mathbb{R}_+$ √© o par√¢metro.
Suponha que estamos interessados em $g(\theta) = \mu$.
Um estimador para $\mu$ √© $\phi(X_1, \dots, X_n) = X_1$ ou $\phi(X_1, \dots, X_n) = \min\{X_i\}$. 
Todavia, um estimador com melhores propriedades, que veremos ao decorrer do curso, √© $\phi(X_1, \dots, X_n) = \bar{X}$. 
Note que 
$$
\mathbb{E}_{\mu}[\bar{X}] = \frac{1}{n}\sum_{i=1}^n \mathbb{E}_{\mu}[X_i] = \mu,
$$
o que mostra que $\bar{X}$ √© n√£o enviesado.

---

Aparentemente, um estimador ser n√£o enviesado parece muito bom, n√£o √©? Mas nem sempre isso √© suficiente ou poss√≠vel. 
No exemplo a seguir, mostramos que n√£o existe estimador n√£o enviesado para a taxa da distribui√ß√£o exponencial.

---
``üìù`` **Exemplo (Exponencial)**

Seja $X \sim Exponencial(\lambda)$, em que $\lambda \in \mathbb{R}_+$ √© o par√¢metro. 
Seja $\phi(X)$ um estimador n√£o enviesado para $\lambda$.
Ent√£o
$$
\mathbb{E}_{\lambda}[\phi(X)] = \lambda \int_0^{+\infty} \phi(x) \exp(-\lambda x) \, dx = \lambda, \forall \lambda \in \mathbb{R}_+. 
$$
Dividindo ambos os lados por $\lambda$ e diferenciando com respeito a $\lambda$, pela [Regra de Leibniz](https://en.wikipedia.org/wiki/Leibniz_integral_rule):
$$
- \int_0^{+\infty} x\phi(x) \exp(-\lambda x) \, dx = 0.
$$
Mas $x\exp(-\lambda x) > 0, \forall x > 0$. 
Se $\phi(x) > 0$ em um conjunto de medida positiva, pelo que estudamos em [Integra√ß√£o](https://lucasmoschen.github.io/ta-sessions/infestatistica_MSc/probability/#integracao), ter√≠amos que a integral deveria ser positiva, o que √© um absurdo. 
Com isso $\phi(x) = 0$ quase certamente e, portanto, n√£o √© um estimador n√£o enviesado. 
Conclu√≠mos que n√£o pode haver um estimador n√£o enviesado para $\lambda$.

---

Um estimador ser n√£o enviesado implique que, na pr√°tica, se fossem feitos infinitos experimentos e calcul√°ssemos o valor do estimador, o valor m√©dio convergiria para o valor verdadeiro do par√¢metro. 
Isso ocorre pela Lei dos Grandes N√∫meros.
Mas na pr√°tica, fazemos uma quantidade finita de experimentos e isso pode impactar nossos resultados. 
No pr√≥ximo exemplo, vamos mostras que um estimador n√£o enviesado e vamos visualizar sua distribui√ß√£o.

---
``üìù`` **Exemplo (Correla√ß√£o normal multivariada)**

Seja $(X_1, Y_1), \dots, (X_n,Y_n) \sim Normal(\boldsymbol{\mu}, \Sigma)$, com m√©dia $\mu = (0,0)$ e matriz de correla√ß√£o $\Sigma = [[1, \rho], [\rho, 1]]$. 
Assim o par√¢metro de interesse √© a correla√ß√£o entre $X$ e $Y$ $\rho \in [-1,1]$.
Considere o estimador para $\rho$ para $n$ amostras sendo
$$
\phi(X_1, \dots, X_n) = \dfrac{\sum_{i=1}^n (X_i - \bar{X})(Y_i - \bar{Y})}{\sqrt{\sum_{i=1}^n (X_i - \bar{X})^2}\sqrt{\sum_{i=1}^n (Y_i - \bar{Y})^2}},
$$
conhecido como **correla√ß√£o emp√≠rica** ou **correla√ß√£o de Pearson**.
Apesar de famoso, esse estimador tem vi√©s n√£o nulo. 
Todavia, pode-se corrigi-lo para obtermos um estimador n√£o enviesado [(Olkin and Pratt, 1958)](https://projecteuclid.org/journals/annals-of-mathematical-statistics/volume-29/issue-1/Unbiased-Estimation-of-Certain-Correlation-Coefficients/10.1214/aoms/1177706717.full),
da forma
$$
\psi = G(\phi) = \phi F\left(\frac{1}{2}, \frac{1}{2}; (n-1)/2; 1 - \phi^2\right),
$$
que tem a propriedade de n√£o ser enviesado, em que $F$ √© a fun√ß√£o hipergeom√©trica de Gauss.
Como $\phi$ e $\psi$ s√£o fun√ß√µes mensur√°veis das amostras, eles tamb√©m t√™m uma distribui√ß√£o amostral.
Uma maneira de visualizar essas distribui√ß√µes √© atrav√©s de simula√ß√µes de Monte Carlo.

1) Geramos $n$ amostras $(X_i, Y_i)$ da normal multivariada.
2) Calculamos $\phi$ e $\psi$. 
3) Repetimos esse processo $M$ vezes e temos amostras da distribui√ß√£o de $\phi$ e $\psi$.

```python
# Par√¢metros
rho = 0.1
n = 30
M = 100000
phi_samples = np.zeros(M)
psi_samples = np.zeros(M)
# Gerando os dados de acordo com a distribui√ß√£o especificada
rng = np.random.RandomState(1001)
for i in range(M):
    Z = rng.multivariate_normal(mean=[0,0], cov=[[1,rho], [rho,1]], size=n)
    X = Z[:,0]
    Y = Z[:,1]
    # Calculando ~ correla√ß√£o de Pearson
    phi_samples[i] = np.corrcoef(X,Y)[0,1]
    psi_samples[i] = phi_samples[i] * hyp2f1(1/2, 1/2, (n-1)/2, 1 - phi_samples[i]**2)

# Desenhando as distribui√ß√µes
plt.hist(phi_samples, label=r'$\phi$', color='pink', bins=50)
plt.hist(psi_samples, label=r'$\psi$', color='blue', bins=50)
plt.legend()
plt.show()
```

![png](output_3_0.png)
    

Nesse caso, temos que o Erro Absoluto Percentual M√©dio do estimador $\psi$ √©, aproximadamente,  
$$
\mathbb{E}_{\rho}\left[\bigg|\frac{\psi - \rho}{\rho}\bigg|\right] \approx 150\%.
$$
Ou seja, apesar de $\psi$ ser n√£o enviesado e $\phi$ ter vi√©s baixo, a vari√¢ncia dos estimadores √© bem grande. 
Nesse caso, temos uma probabilidade alta de obter uma estima√ß√£o 100% maior ou menor do que o valor verdadeiro de $\rho$.

Note que a diferen√ßa entre os estimadores √© pequena, pois $n$ √© razoavelmente grande e $\psi$ tem vi√©s baixo.

---

### Erro quadr√°tico

Seja $L(\theta, d) = (\theta - d)^2$ a perda quadr√°tica.
A [fun√ß√£o de risco](https://lucasmoschen.github.io/ta-sessions/infestatistica_MSc/risk_function/) $R(\theta, \phi)$ para o estimador $\phi(X)$ √© dada por 
$$
\begin{split}
R(\theta, \phi) &= \mathbb{E}_{\theta}[(\theta-\varphi(X))^2] \\
&= \mathbb{E}_{\theta}[(\theta-\mathbb{E}[\varphi(X)] + \mathbb{E}[\varphi(X)] - \varphi(X))^2] \\
&= (\theta-\mathbb{E}[\varphi(X)])^2 + 2(\theta-\mathbb{E}[\varphi(X)])(\mathbb{E}_{\theta}[\mathbb{E}[\varphi(X)] - \varphi(X))] + \mathbb{E}_{\theta}[(\mathbb{E}[\varphi(X)] - \varphi(X))^2] \\ 
&= b_{\phi}(\theta)^2 + 2(\theta-\mathbb{E}[\varphi(X)])(\mathbb{E}[\varphi(X)]-\mathbb{E}[\varphi(X)]) + \operatorname{Var}(\phi(X)) \\
&= b_{\phi}(\theta)^2 + \operatorname{Var}(\phi(X)).
\end{split}
$$
Portanto, para estimadores n√£o enviesados, o risco √© dado somente pela vari√¢ncia do estimador.

## Estima√ß√£o n√£o enviesada de menor vari√¢ncia

Note que quando queremos avaliar o melhor estimador, estamos lidando com uma classe bem grande para comparar. 
Por exemplo, o estimador $\hat{\theta} = 0$ para todo $\theta$ tem o menor MSE em $\theta = 0$, mas √© um estimador horr√≠vel quando $\theta$ se afasta de zero.
Nesse, sentido uma forma de avaliar estimadores √© restringindo a classe de interesse. 
Uma classe considerada razo√°vel √© a dos estimadores n√£o enviesados.
Nesse caso, os MSEs ser√£o iguais √†s vari√¢ncias dos estimadores.

> Um estimador n√£o enviesado $\phi$ √© **uniformly minimum variance unbiased estimator (UMVUE)** (estimador n√£o enviesado de vari√¢ncia uniformemente m√≠nima) se $\phi$ tem vari√¢ncia finita e, para todo estimador n√£o enviesado, $Var_{\theta} \phi(X) \le \operatorname{Var}_{\theta} \psi(X), \forall \theta \in \Omega$.

Essa defini√ß√£o leva ao seguinte importante teorema:

**Teorema (Lehmann-Scheff√©):** Se $T$ √© uma estat√≠stica completa, ent√£o todos os estimadores n√£o enviesados de $g(\theta)$, que s√£o fun√ß√µes de $T$, mas n√£o de $X$, s√£o iguais quase certamente para todo $\theta$.
Al√©m disso, se existe um estimador n√£o enviesado que √© fun√ß√£o de uma estat√≠stica suficiente completa, ent√£o ele √© UMVUE.

---
Ideia da prova: Sejam $\phi_1(T)$ e $\phi_2(T)$ estimadores n√£o enviesados de $g(\theta)$. 
Ent√£o $\mathbb{E}_{\theta}[\phi_1(T) - \phi_2(T)] = 0$ para todo $\theta$.
Como $T$ √© uma estat√≠stica completa, vale que $\phi_1(T) = \phi_2(T)$ quase certamente.
Agora suponha que $T$ √© estat√≠stica suficiente completa e seja $\phi(T)$ um estimador n√£o enviesado.
Tome um estimador n√£o enviesado $\psi(X)$ e defina $\phi'(T) = \mathbb{E}[\psi(X)|T]$. Usando a perda quadr√°tica, pelo [Teorema de Rao-Blackwell](https://lucasmoschen.github.io/ta-sessions/infestatistica_MSc/sufficiency/sufficiency/#teorema-de-rao-blackwell), $\operatorname{Var}_{\theta}(\phi ') = R(\theta, \phi ') \le R(\theta, \psi) = \operatorname{Var}_{\theta}(\psi)$ para todo $\theta$.
Portanto $\phi '$ √© UMVUE. Al√©m disso, como acabamos de provar, $\phi = \phi '$ quase certamente e, portanto, tamb√©m √© UMVUE.

---

Note que basta a exist√™ncia de um estimador n√£o enviesado $\delta$ e de uma estat√≠stica completa $T$ para que encontremos um estimador UMVUE usando Rao-Blackwell:
$$
\phi(T) = \mathbb{E}_{\theta}[\delta(X) | T].
$$

Considere o exemplo para o caso normal.

---
``üìù`` **Exemplo (Distribui√ß√£o normal - ela de novo)**

Seja $X_1, \dots, X_n \overset{iid}{\sim} N(\mu, \sigma^2)$ e $\theta = (\mu, \sigma^2)$.
Ent√£o
$$
\bar{X} = \frac{1}{n}\sum_{i=1}^n X_i, S^2 = \frac{1}{n-1}\sum_{i=1}^n (X_i - \bar{X})^2
$$
s√£o estat√≠sticas suficientes e completas, al√©m de serem estimadores n√£o enviesados para $\mu$ e $\sigma^2$ respectivamente.
Pelo Teorema acima, qualquer fun√ß√£o deles √© um UMVUE pelo resultado acima, para $\mu$ e $\sigma^2$, respectivamente.
Todavia, √© poss√≠vel verificar que $S^2$ n√£o minimiza o erro quadrado.

*Obs.: para mostrar a sufici√™ncia, o Teorema da Fatoriza√ß√£o √© suficiente. J√° a completude vem do fato de que o espa√ßo de par√¢metros natural cont√©m um conjunto aberto em $\mathbb{R}^2$*.

---

Quando n√£o existem estat√≠sticas completas, o seguinte resultado pode ser √∫til:

**Teorema:** Uma condi√ß√£o suficiente e necess√°ria para que um estimador $\delta$ de $\mathbb{E}_{\theta} \delta(X)$ seja UMVUE √© que para todo $U$ que satisfa√ßa $\forall \theta, \mathbb{E}_{\theta}[U] = 0$, valha que $\operatorname{Cov}_{\theta}(\delta(X), U(X)) = 0$

---
``üìù`` **Exemplo (Bernoulli)**

Seja $X_1, \dots, X_n \sim \overset{iid}{\sim} Bernoulli(\theta)$.
Temos que $T(X) = \sum_{i=1}^n X_i$ √© uma estat√≠stica suficiente completa para $\theta$.
Al√©m disso, sabemos que $T(X) \sim Binomial(n,\theta)$.
Em particular, $T$ √© estat√≠stica completa, pois o espa√ßo de par√¢metros natural da distribui√ß√£o binomial tem interior n√£o vazio.
Considere $g(\theta) = \theta^2$. 
Um estimador n√£o enviesado para $g$ √© $\delta(X) = X_1 X_2$.
Nesse caso, o estimador UMVUE de $g$ √© $\mathbb{E}[\delta(X)|T]$, isto √©, 
$$
\phi(T) = \mathbb{E}_{\theta}[X_1X_2|T] = \mathbb{P}_{\theta}(X_1 = 1, X_2 = 1 | T = t).
$$
Podemos calcular essa probabilidade pela defini√ß√£o de probabilidade condicional, mas vamos fazer isso intuitivamente.
Temos $n$ espa√ßos e $t$ bolinhas para preencher $t$ desses espa√ßos. 

- Quantas formas eu tenho de posicionar as bolinhas? $n \choose t$. 

- Quantas dessas formas eu tenho interesse? Posiciono duas bolinhas nas primeiras posi√ß√µes (veja que preciso de $t \ge 2$) e tenho $n-2 \choose t-2$ formas de posicionar as outras bolinhas. Portanto:

$$
\mathbb{P}_{\theta}(X_1 = 1, X_2 = 1 | T = t) = \frac{(n-2)(n-3)\cdots(n-t+1)\cdot t(t-1)\cdots 1}{(t-2)(t-1)\cdots 1 \cdot n(n-1)\cdots(n-t+1)} = \frac{t(t-1)}{n(n-1)}.
$$

Portanto, o estimador $\phi(T) = T(T-1)/(n^2 - n)$ se $T \ge 2$ e $\phi(T) = 0$ se $T \le 1$ √© UMVUE para $g(\theta) = \theta^2$.

---

## Estimador de M√°xima Verossimilhan√ßa

- Cap 5.1.3 Schervish (307 - 309)
- Cap 7.2.2 Casella (315 - 324)

## M√©todo dos Momentos 

- Cap 7.2.1 Cassela (312 - 315)

Obs.: incluir alguns pensamentos de cap 7.3.3 Cassella (342 - 348)

### Distribui√ß√£o assint√≥tica de um estimador eficiente 

Assuma as hip√≥teses do teorema de Cram√©r-Rao. Seja $T$ um estimador eficiente para a sua m√©dia $m(\theta)$ e $m'(\theta) \neq 0$. Ent√£o: 

$$
\frac{[nI(\theta)]^{1/2}}{m'(\theta)}[T - m(\theta)] \overset{d}{\to} N(0,1)
$$

### Distribui√ß√£o assint√≥tica do MLE

Suponha que obtemos $\hat{\theta}_n$ resolvendo a equa√ß√£o $\lambda_n'(x|\theta) = 0$, isto √©, maximizando a log-verossimilhan√ßa (MLE). E suponha que $\lambda_n''$ e $\lambda_n'''$ existem e satisfazem certas condi√ß√µes de regularidade. Ent√£o 

$$
[nI(\theta)]^{1/2}(\hat{\theta}_n - \theta) \overset{d}{\to} N(0,1)
$$

Como o MLE √© n√£o enviesado, ent√£o se ele for Eficiente, j√° sabemos que esse teorema √© verdade pelo anterior. (se ele √© n√£o enviesado)

# Introdu√ß√£o a grandes amostras

Agora, vamos verificar algumas propriedades assint√≥ticas, isto √©, quando o n√∫mero de amostras √© muito grande, com $n \to \infty$.

## Conceitos de converg√™ncia

Uma lista de conceitos de converg√™ncia importantes. 
Em particular, o conceito de **consist√™ncia** √© importante para estimadores, dado que gostar√≠amos que, com amostras suficientes, tiv√©ssemos valores razo√°veis para o par√¢metro. 

1. **Determin√≠stica:** Seja $\{x_n\}_{n \in \mathbb{N}}$ uma sequ√™ncia em um espa√ßo normado e  $\{r_n\}_{n \in \mathbb{N}}$ uma sequ√™ncia de reais.
Se para cada $c > 0$, existe $N$ suficientemente grande tal que $||x_n|| \le c|r_n|$ para $n \ge N$, dizemos que $x_n = o(x_n)$.
Se existe $c > 0$ tal que a desigualdade anterior valha para $n$ grande, ent√£o dizemos que $x_n = O(r_n)$.

2. **Estoc√°stica:**  Seja $\{X_n\}_{n \in \mathbb{N}}$ uma sequ√™ncia de vari√°veis aleat√≥rias definidas em espa√ßos normados e  $\{r_n\}_{n \in \mathbb{N}}$ uma sequ√™ncia de reais.
Se para cada $c > 0$ e $\epsilon > 0$, existe $N$ tal que $\mathbb{P}(||X_n|| \le c|r_n|) \ge 1 - \epsilon$ para $n \ge N$, dizemos que $X_n = o_P(x_n)$.
Se para cada $\epsilon >0$, existe $c > 0$ tal que a desigualdade anterior valha para $n$ grande, ent√£o dizemos que $X_n = O_P(r_n)$.

3. **Converg√™ncia em probabilidade:** Se $\{X_n\}_{n \in \mathbb{N}}$ e $X$ s√£o quantidades aleat√≥rias e, para todo $\epsilon >0$, vale que 
$$
\lim_{n \to \infty} Pr(||X_n - X|| > \epsilon) = 0,
$$
ent√£o dizemos que $X_n \overset{P}{\to} X$, isto √©, $X_n$ converge em probabilidade para $X$.

Se $Y_n = f_n(X_n)$ para uma sequ√™ncia de fun√ß√µes mensur√°veis $f_n$ e $Y$ √© uma outra quantidade aleat√≥ria, temos que $||Y_n - Y|| = o_P(1)$ se, e s√≥ se, $Y_n \overset{P}{\to} Y$. 
Em particular, se $f$ √© cont√≠nua em $c$ e $X_n \overset{P}{\to} c$, ent√£o $f(X_n) \overset{P}{\to} f(c)$.

4. **Consist√™ncia:** Sejam $g$ uma fun√ß√£o mensur√°vel e $P_{\theta}$ uma distribui√ß√£o param√©trica definida em $\mathcal{X}$. 
A sequ√™ncia de vari√°veis aleat√≥rias $Y_n : \mathcal{X}^n \to G$ √© consistente para $g(\theta)$ se $Y_n \overset{P}{\to} g(\theta)$ para todo $\theta \in \Omega$.
A **lei dos grandes n√∫meros** √© um forte aliado, pois afirma que a m√©dia amostral converge em probabilidade para a m√©dia verdadeira.

5. **Converg√™ncia em distribui√ß√£o:** Seja $\{X_n\}$ uma sequ√™ncia de quantidades aleat√≥rias e $X$ uma quantidade aleat√≥ria.
Se 
$$
\lim_{n \to \infty} \mathbb{E}[f(X_n)] = \mathbb{E}[f(X)],
$$
para toda fun√ß√£o cont√≠nua limitada $f$, dizemos que $X_n$ converge em distribui√ß√£o para $X$, isto √©, $X_n \overset{D}{\to} X$ ou $X_n \Rightarrow X$.
Dizemos que a distribui√ß√£o de $X$ √© a **distribui√ß√£o assint√≥tica** de $X_n$.
Al√©m do mais, dizemos que a distribui√ß√£o de $X_n$ converge fracamente para a distribui√ß√£o de $X$.
Se $H_n(x)$ √© a fun√ß√£o de distribui√ß√£o acumulada de $X_n$ e $H$ √© a CDF de $X$, ent√£o $H_n(x) \to H(x)$  sempre que $H$ √© cont√≠nua em $x$ se, e somente se, $X_n \overset{D}{\to} X$

**Teorema:** Se $Y_n \Rightarrow Y$, $A_n \overset{P}{\to} a$ e $B_n \overset{P}{\to} b$, ent√£o
$$
A_n + B_nY_n \Rightarrow a + bY.
$$

6. **Converg√™ncia quase certa:** Uma sequ√™ncia $\{X_n\}$ converge quase certamente para $X$ se $\mathbb{P}(Y_n \to Y) = 1$.

## Consist√™ncia

O exemplo cl√°ssico de consist√™ncia √© o seguinte:

---
``üìù`` **Exemplo (Lei fraca dos grandes n√∫meros)**

Seja $X_1, X_2, \dots$ uma sequ√™ncia de vari√°veis aleat√≥rias independentes cuja distribui√ß√£o tem densidade $f(x|\theta)$.
Defina $g(\theta) = \mathbb{E}_{\theta}[X]$ e $\bar{X}_n = n^{-1}(X_1 + \dots + X_n)$.
Pela lei fraca dos grandes n√∫meros, temos que $\{\bar{X}_n\}$ √© uma sequ√™ncia de estimadores consistente para $g(\theta)$.

---

**Teorema:** Seja $\{W_n\}$ uma sequ√™ncia de estimadores para $\theta$ tal que, para todo $\theta \in \Omega$,

(i) $\lim_{n \to \infty} \operatorname{Var}_{\theta}(W_n) = 0$,

(ii) $\lim_{n \to \infty} \operatorname{Bias}_{\theta}(W_n) = 0$.

Ent√£o $W_n$ √© sequ√™ncia de estimadores consistente de $\theta$.
Esse resultado √© consequ√™ncia direto do fato de que 
$$
\mathbb{E}_{\theta}[(W_n - \theta)^2] = \operatorname{Var}_{\theta}(W_n) + \operatorname{Bias}_{\theta}(W_n)^2
$$
e da desigualdade de Chebyshev, 
$$
P_{\theta}(|W_n - \theta| \ge \epsilon) \le \frac{\mathbb{E}_{\theta}[(W_n - \theta)^2]}{\epsilon^2}.
$$

Al√©m do mais, se fizemos $U_n = a_n W_n + b_n$, com $a_n \to 1$ e $b_n \to 0$, temos que $U_n$ tamb√©m √© consistente.

## Teorema Central do Limite

Seja $X_1, \dots, X_n$ vari√°veis aleat√≥rias iid com m√©dia $\mu$ e vari√¢ncia $\sigma^2$. 
Ent√£o
$$
\sqrt{n}(\bar{X}_n - \mu) \Rightarrow N(0, \sigma^2). 
$$

## Propriedades assint√≥ticas MLE

O seguinte Teorema afirma que sob o modelo $f(x|\theta_0)$, a probabilidade de que a fun√ß√£o de verossimilhan√ßa seja estritamente maior em $\theta_0$ tende a $1$ quando $n \to \infty$. 
Nesse sentido, se $n$ √© suficientemente grande, nossa probabilidade de que sob aquela amostra a verossimilhan√ßa seja maior do que qualquer outro ponto √© pr√≥ximo a 1. 

**Teorema:** Seja $f_n(x|\theta)$ a densidade de uma amostra aleat√≥ria $(X_1, \dots, X_n)$.
Ent√£o, para cada $\theta_0, \theta \in \Omega$ com $\theta \neq \theta_0$, vale que 
$$
\lim_{n \to \infty} P_{\theta_0}\left[f_n(x|\theta_0) > f_n(x|\theta)\right] = 1.
$$

Agora, vamos verificar que o MLE √© um estimador consistente.

**Teorema:** Seja $f_n(x|\theta)$ a densidade de uma amostra aleat√≥ria $(X_1, \dots, X_n)$ e fixe $\theta_0 \in \Omega$. 
Para cada $M\subseteq \Omega$ e $x \in \mathcal{X}$, defina
$$
Z(M,x) = \inf_{\psi \in M} \log \frac{f(x|\theta_0)}{f(x|\psi)} = \log f(x|\theta_0) - \sup_{\psi \in \Omega} \log f(x|\psi).
$$
Assuma que para cada $\theta \neq \theta_0$, exista uma vizinhan√ßa $N_{\theta}$ tal que $\mathbb{E}_{\theta_0}[Z(N_{\theta}, X_i)] > 0$.
Se $\Omega$ n√£o √© compacto, assuma que exista um compacto $C \subseteq \Omega$ tal que $\theta_0 \in C$ e $\mathbb{E}_{\theta_0} Z(\omega / C, X_i) > 0$. 
Ent√£o
$$
\lim_{n \to \infty} \hat{\theta}_n = \theta_0,
$$
quase certamente.

A fun√ß√£o $Z(M,x)$ √© a diferen√ßa da log-verossimilhan√ßa em $\theta_0$ e o m√°ximo que ela atinge em $M$. 
Estamos assumindo que em uma vizinhan√ßa de cada ponto $\theta \in \Omega$, a m√°xima verossimilhan√ßa nessa regi√£o √©, em m√©dia, menor do que a verossimilhan√ßa em $\theta_0$.
Al√©m disso, para conjuntos n√£o compactos, fora desse compacto, queremos que em m√©dia a verossimilhan√ßa em $\theta_0$ seja maior.
A dificuldade de utilizar esse teorema √© verificar todas essas condi√ß√µes.

Assumindo a continuidade da fun√ß√£o de verossimilhan√ßa para todo $x$ e que $\mathbb{E}_{\theta_0}[Z(N_{\theta}, X_i)] > -\infty$, tamb√©m temos que MLE √© um estimador consistente.

### MLE para a fam√≠lia exponencial

Seja $\hat{\theta}_n$ MLE calculado a partir de $X_1, \dots, X_n$ cuja distribui√ß√£o tem densidade 
$$
f(x|\theta) = h(x)\exp\{\theta\cdot x - A(\theta)\},
$$
isto √©, pertence √† fam√≠lia exponencial na forma can√¥nica.
Se $\Omega$ √© um subconjunto aberto de $\mathbb{R}^k$, ent√£o

- $\lim_{n\to\infty} \mathbb{P}_{\theta}(\hat{\theta}_n \text{ existir}) = 1$

- $\sqrt{n}(\hat{\theta}_n - \theta) \overset{D}{\to} N_k(0, I_{\mathcal{X}}(\theta))^{-1}$, em que $I_{\mathcal{X}}(\theta)$ √© a matriz informa√ß√£o de Fisher.

- $\{\hat{\theta}_n\}$ √© consistente para $\theta$.

### Normalidade assint√≥tica para MLEs

Sob algumas hip√≥teses de regularidade, podemos concluir que 
$$
\sqrt{n}(\hat{\theta}_n - \theta_0) \overset{D}{\to} N(0, I_{\mathcal{X}}(\theta_0)^{-1}),
$$
em que $I_{\mathcal{X}}(\theta_0)$ √© a informa√ß√£o de Fisher.
Hip√≥teses:

1. $\{\hat{\theta}_n\}$ √© consistente para $\theta$.
2. $f(x|\theta)$ tem derivadas de segunda ordem cont√≠nuas com respeito a $\theta$ e vale o sinal de integra√ß√£o pode ser trocado com o de diferencia√ß√£o.
3. Existe uma fun√ß√£o $H_r(x,\theta)$ que para todo $\theta_0$ tem valor esperando nulo quanto $r \to 0$ e seja limite superior da diferen√ßa na segunda derivada em uma bola de raio $r$ de $\theta_0$.

Na pr√°tica, podemos usar $I_{\mathcal{X}}(\hat{\theta}_n)$, visto que $\theta_0$ √© desconhecido.
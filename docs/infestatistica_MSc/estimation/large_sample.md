# Introdu√ß√£o a grandes amostras

Agora, vamos verificar algumas propriedades assint√≥ticas, isto √©, quando o n√∫mero de amostras √© muito grande, com $n \to \infty$.

## Conceitos de converg√™ncia

Uma lista de conceitos de converg√™ncia importantes. 
Em particular, o conceito de **consist√™ncia** √© importante para estimadores, dado que gostar√≠amos que, com amostras suficientes, tiv√©ssemos valores razo√°veis para o par√¢metro. 

1. **Determin√≠stica:** Seja $\{x_n\}_{n \in \mathbb{N}}$ uma sequ√™ncia em um espa√ßo normado e  $\{r_n\}_{n \in \mathbb{N}}$ uma sequ√™ncia de reais.
Se para cada $c > 0$, existe $N$ suficientemente grande tal que $||x_n|| \le c|r_n|$ para $n \ge N$, dizemos que $x_n = o(x_n)$.
Se existe $c > 0$ tal que a desigualdade anterior valha para $n$ grande, ent√£o dizemos que $x_n = O(r_n)$.

2. **Estoc√°stica:**  Seja $\{X_n\}_{n \in \mathbb{N}}$ uma sequ√™ncia de vari√°veis aleat√≥rias definidas em espa√ßos normados e  $\{r_n\}_{n \in \mathbb{N}}$ uma sequ√™ncia de reais.
Se para cada $c > 0$ e $\epsilon > 0$, existe $N$ tal que $P(||X_n|| \le c|r_n|) \ge 1 - \epsilon$ para $n \ge N$, dizemos que $X_n = o_P(x_n)$.
Se para cada $\epsilon >0$, existe $c > 0$ tal que a desigualdade anterior valha para $n$ grande, ent√£o dizemos que $X_n = O_P(r_n)$.

3. **Converg√™ncia em probabilidade:** Se $\{X_n\}_{n \in \mathbb{N}}$ e $X$ s√£o quantidades aleat√≥rias e, para todo $\epsilon >0$, vale que 
$$
\lim_{n \to \infty} Pr(||X_n - X|| > \epsilon) = 0,
$$
ent√£o dizemos que $X_n \overset{P}{\to} X$, isto √©, $X_n$ converge em probabilidade para $X$.

Se $Y_n = f_n(X_n)$ para uma sequ√™ncia de fun√ß√µes mensur√°veis $f_n$ e $Y$ √© uma outra quantidade aleat√≥ria, temos que $||Y_n - Y|| = o_P(1)$ se, e s√≥ se, $Y_n \overset{P}{\to} Y$.

4. **Consist√™ncia:** Sejam $g$ uma fun√ß√£o mensur√°vel e $P_{\theta}$ uma distribui√ß√£o param√©trica definida em $\mathcal{X}$. 
A sequ√™ncia de vari√°veis aleat√≥rias $Y_n : \mathcal{X}^n \to G$ √© consistente para $g(\theta)$ se $Y_n \overset{P}{\to} g(\theta)$ para todo $\theta \in \Omega$.
A ***lei dos grandes n√∫meros** √© um forte aliado, pois afirma que a m√©dia amostral converge em probabilidade para a m√©dia verdadeira.

5. **Converg√™ncia em distribui√ß√£o:** Seja $\{X_n\}$ uma sequ√™ncia de quantidades aleat√≥rias e $X$ uma quantidade aleat√≥ria.
Se 
$$
\lim_{n \to \infty} \mathbb{E}[f(X_n)] = \mathbb{E}[f(X)],
$$
para toda fun√ß√£o cont√≠nua limitada $f$, dizemos que $X_n$ converge em distribui√ß√£o para $X$, isto √©, $X_n \overset{D}{\to} X$.
Dizemos que a distribui√ß√£o de $X$ √© a **distribui√ß√£o assint√≥tica** de $X_n$.
Al√©m do mais, dizemos que a distribui√ß√£o de $X_n$ converge fracamente para a distribui√ß√£o de $X$.

## Consist√™ncia

O exemplo cl√°ssico de consist√™ncia √© o seguinte:

---
``üìù`` **Exemplo (Lei fraca dos grandes n√∫meros)**

Seja $X_1, X_2, \dots$ uma sequ√™ncia de vari√°veis aleat√≥rias independentes cuja distribui√ß√£o tem densidade $f(x|\theta)$.
Defina $g(\theta) = \mathbb{E}_{\theta}[X]$ e $\bar{X}_n = n^{-1}(X_1 + \dots + X_n)$.
Pela lei fraca dos grandes n√∫meros, temos que $\{\bar{X}_n\}$ √© uma sequ√™ncia de estimadores consistente para $g(\theta)$.

---

**Teorema:** Seja $\{W_n\}$ uma sequ√™ncia de estimadores para $\theta$ tal que, para todo $\theta \in \Omega$,

(i) $\lim_{n \to \infty} \operatorname{Var}_{\theta}(W_n) = 0$,

(ii) $\lim_{n \to \infty} \operatorname{Bias}_{\theta}(W_n) = 0$.

Ent√£o $W_n$ √© sequ√™ncia de estimadores consistente de $\theta$.
Esse resultado √© consequ√™ncia direto do fato de que 
$$
\mathbb{E}_{\theta}[(W_n - \theta)^2] = \operatorname{Var}_{\theta}(W_n) + \operatorname{Bias}_{\theta}(W_n)^2
$$
e da desigualdade de Chebyshev, 
$$
P_{\theta}(|W_n - \theta| \ge \epsilon) \le \frac{\mathbb{E}_{\theta}[(W_n - \theta)^2]}{\epsilon^2}.
$$

Al√©m do mais, se fizemos $U_n = a_n W_n + b_n$, com $a_n \to 1$ e $b_n \to 0$, temos que $U_n$ tamb√©m √© consistente.

## Propriedades assint√≥ticas MLE

- KN, Cap 8 e 9;
- SV, Cap 7.3.

- Cap 10.1.1 Cassela (467 - 470)
- Cap 8.1 Keener

### Distribui√ß√£o assint√≥tica de um estimador eficiente 

Assuma as hip√≥teses do teorema de Cram√©r-Rao. Seja $T$ um estimador eficiente para a sua m√©dia $m(\theta)$ e $m'(\theta) \neq 0$. Ent√£o: 

$$
\frac{[nI(\theta)]^{1/2}}{m'(\theta)}[T - m(\theta)] \overset{d}{\to} N(0,1)
$$

### Distribui√ß√£o assint√≥tica do MLE

Suponha que obtemos $\hat{\theta}_n$ resolvendo a equa√ß√£o $\lambda_n'(x|\theta) = 0$, isto √©, maximizando a log-verossimilhan√ßa (MLE). E suponha que $\lambda_n''$ e $\lambda_n'''$ existem e satisfazem certas condi√ß√µes de regularidade. Ent√£o 

$$
[nI(\theta)]^{1/2}(\hat{\theta}_n - \theta) \overset{d}{\to} N(0,1)
$$

Como o MLE √© n√£o enviesado, ent√£o se ele for Eficiente, j√° sabemos que esse teorema √© verdade pelo anterior. (se ele √© n√£o enviesado)


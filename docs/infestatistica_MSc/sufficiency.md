# Estat√≠stica suficiente

Um estat√≠stico usa a informa√ß√£o de uma amostra $X_1, \dots, X_n$ para fazer infer√™ncia sobre uma quantidade de interesse $\theta$.
De acordo com Fisher, em "On the Mathematical Foundations of Theoretical Statistics", "o objeto dos m√©todos estat√≠sticos √© a redu√ß√£o dos dados".
Como a quantidade de dados √© incapaz de ser compreendida diretamente pelo c√©rebro, ela √© reduzida por poucas quantidades que representam o todo, pelo menos a informa√ß√£o relevante para o problema. 
Nesse sentido, uma estat√≠stica √© uma forma de representar esses dados, em geral diminuir sua dimens√£o (os pr√≥prios dados formam uma estat√≠stica, ent√£o isso n√£o √© uma condi√ß√£o necess√°ria).
De forma precisa, a defini√ß√£o de **estat√≠stica** √©:

> Seja $(\mathcal{T}, \mathcal{C})$ um espa√ßo mensur√°vel em que $\mathcal{C}$ contenha todos os conjuntos unit√°rios formados pelo elementos de $\mathcal{T}$. Se $T : \mathcal{X} \to \mathcal{T}$ √© mensur√°vel, ent√£o $T$ √© uma estat√≠stica.

Com isso, podemos definir uma **estat√≠stica suficiente** no sentido cl√°ssico (lembrando que temos uma contrapartida bayesiana baseada na posteriori).

> Seja $T: \mathcal{X} \to \mathcal{T}$ uma estat√≠stica e $\mathcal{P}$ uma fam√≠lia de distribui√ß√µes parametrizada por $\theta$ e definida em $(\mathcal{X}, \mathcal{B})$ (espa√ßo amostral com $\sigma$-√°lgebra $\mathcal{B}$). Suponha que existem $P_{\theta}(\cdot|T)$ e uma fun√ß√£o $r : \mathcal{B} \times \mathcal{T} \to [0,1]$ tal que $r(\cdot, t)$ √© uma medida de probabilidade em $(\mathcal{X}, \mathcal{B})$ para todo $t \in \mathcal{T}$, $r(A, \cdot)$ √© mensur√°vel para todo $A \in \mathcal{B}$ e, para todo $\theta \in \Theta$ e $B \in \mathcal{B}$,
> $$
P_{\theta}(B|T=t) = r(B,t)
> $$
> quase certamente. Ent√£o dizemos que $T$ √© **estat√≠stica suficiente** para $\theta$.

Apesar dessa defini√ß√£o ser bastante complexa, a ideia √© que a distribui√ß√£o condicional de $X$ dado $T=t$ n√£o depende de $\theta$, isto √©, a informa√ß√£o trazida por $T=t$ sobre $\theta$ compreende toda a informa√ß√£o dispon√≠vel de $X$.
Ap√≥s observar $T(x) = t$, podemos amostrar de $r(\cdot, t)$ e teremos dados falsos que imitam os iniciais, pois a distribui√ß√£o ser√° a mesma.
Assim, se considerarmos dois experimentadores, um tendo a amostra inteira, e o outro tendo apenas uma estat√≠stica suficiente, ambos ter√£o a mesma quantidade de informa√ß√£o sobre o par√¢metro de interesse $\theta$.

Note que, pela lei da esperan√ßa total,
$$
P_{\theta}(X \in B) = \mathbb{E}_{\theta}[P_{\theta}(X \in B|T)] = \mathbb{E}_{\theta}[r(B, T)].
$$
Se tomarmos $Y \sim r(\cdot, t)$ quando $T=t$, teremos 
$$
P_{\theta}(Y \in B) = \mathbb{E}_{\theta}[P_{\theta}(Y \in B|T)] = \mathbb{E}_{\theta}[r(B, T)].
$$

**Proposi√ß√£o:** A estat√≠stica de ordem $(X_{(1)}, \dots, X_{(n)})$ √© estat√≠stica suficiente.

## Fatoriza√ß√£o de Fisher-Neyman

Seja $f(\boldsymbol{x}|\theta)$ a densidade de uma distribui√ß√£o de $\boldsymbol{X} = (X_1, \dots, X_n)$ (com respeito a uma medida $\nu$ $\sigma$-finita, como a medida de Lebesgue em $\mathbb{R}^n$). Ent√£o $T(\boldsymbol{X})$ √© estat√≠stica suficiente para $\theta$ se, e somente se, existem fun√ß√µes $m_1$ e $m_2$ tal que 
$$
f(\boldsymbol{x}|\theta) = m_1(\boldsymbol{x})m_2(T(\boldsymbol{x}) , \theta), \forall \theta \in \Theta.
$$

O lema 2.24 demonstrado no livro de Schervish determina que sob as hip√≥teses do teorema de Fisher-Neyman e assumindo que $T$ √© suficiente, obtemos que existe uma medida em $(\mathcal{T}, \mathcal{C})$ que domina a distribui√ß√£o de probabilidade de $T$ e define a densidade de $T=t$ sob o par√¢metro $\theta$ como $m_2(t,\theta)$.

## Estat√≠stica suficiente m√≠nima e completa

A estat√≠stica de ordem √© uma estat√≠stica que pouco reduz a informa√ß√£o do dado. 
De fato, s√≥ retira a quest√£o da ordena√ß√£o. 
Todavia, algumas vezes uma estat√≠stica mais simples tamb√©m √© suficiente e, por isso, faz sentido definir quando ela √© m√≠nima.

> Uma estat√≠stica suficiente $T$ √© dita **suficiente m√≠nima** se para toda estat√≠stica $U$, existe uma fun√ß√£o mensur√°vel $g$ tal que $T = g(U)$ para todo $\theta$.

**Teorema (Lehmann-Scheff√©):** Seja $f(x|\theta)$ a densidade e $T$ uma fun√ß√£o mensur√°vel tal que $T(x) = T(y) \iff y \in D(x)$, com
$$
D(x) = \{y \in \mathcal{X} : f(y|\theta) = f(x|\theta)h(x,y), \forall \theta \text{ e alguma fun√ß√£o } h(x,y) \}.
$$

---
``üìù`` **Exemplo**
}
Seja $X_1, \dots, X_n \overset{iid}{\sim} \operatorname{Bernoulli}(\theta)$. Temos que 
$$
f(x_1,\dots,x_n|\theta) = \theta^{S_x}(1-\theta)^{n-S_x},
$$
em que $S_x = \sum_{i=1}^n x_i$. 
Assim
$$
\frac{f(x_1,\dots,x_n|\theta)}{f(y_1,\dots,y_n|\theta)} = \left(\frac{\theta}{1-\theta}\right)^{S_x-S_y},
$$
que independe de $\theta$ se, e somente se, $S_x = S_y$. 
Isso mostra que $T(x) = S_x$ √© estat√≠stica suficiente m√≠nima.

---

---
``üìù`` **Exemplo - Fam√≠lia Exponencial**

Considere uma distribui√ß√£o com densidade 
$$
p_{\theta}(x) = \exp\{\eta(\theta)\cdot T(x) - B(\theta)\}h(x)
$$
com respeito a medida de Lebesgue. 
Pelo Teorema da Fatoriza√ß√£o $T(x)$ √© estat√≠stica suficiente.
Assim
$$
\log\left(\frac{p_{\theta}(y)}{p_{\theta}(x)}\right) = \eta(\theta)\cdot (T(y) - T(x)) + \log(h(y)) - \log(h(x)).
$$
Logo $T$ √© estat√≠stica suficiente m√≠nima se, e somente se, o complemento ortogonal de $\eta(\theta)$ possui apenas o $0$.

---

> Uma estat√≠stica $T$ √© dita **completa** se para toda fun√ß√£o $g$ mensur√°vel e $\theta \in \Theta$, $\mathbb{E}_{\theta}[g(T)] = 0$ implique $g(T) = 0$. Ela ser√° **completa limitada** se adicionamos a condi√ß√£o de que $g$ √© limitada.

**Teorema de Bahadur:** Se $U$ √© uma estat√≠stica suficiente completa limitada e de dimens√£o finita, ent√£o ela √© suficiente m√≠nima.

Ideias da demonstra√ß√£o:

- Tome uma outra estat√≠stica $T$ e defina $V_i(U) = (1 + \exp\{U_i\})^{-1}$ para cada componente de $U$. Note que $V$ √© limitada.
- Defina $H_i(t)$ como o valor esperado de $V_i$ dado que $T=t$ e $L_i(u)$ como o valor esperado de  $H_i(T)$ quando $U=u$, que n√£o dependem de $\theta$, pois as estat√≠sticas s√£o suficientes.
- Veja que $\mathbb{E}_{\theta}[V_i(U) - L_i(U)] = 0$, o que implica que $\mathbb{P}(V_i = L_i) = 1$ pois $U$ √© completa.
- Conclua usando a Lei da Vari√¢ncia Total que $U_i = V_i^{-1}(H_i(T))$.

Note que se $U$ √© completa, ent√£o $U$ √© completa limitada, o que nos d√° um resultado mais claro: Uma estat√≠stica suficiente e completa √© suficiente m√≠nima.

## Ancilaridade

Na outra ponta, temos estat√≠sticas que s√£o independentes do par√¢metro.

> Uma estat√≠stica $U$ √© dita anciliar se a sua distribui√ß√£o independe de $\theta$.

---
``üìù`` **Exemplo - Caso normal**

Sejam $X_1, X_2 \overset{iid}{\sim} \operatorname{Normal}(\mu, 1)$ e defina $U = X_2 - X_1$. Soma de normais independentes faz com que 
$U \sim \operatorname{Normal}(0, 2)$ e $U$ √© anciliar.

---

Se $T=(T_1, T_2)$ √© estat√≠stica suficiente m√≠nima e $T_2$ √© anciliar, dizemos que $T_1$ √© **suficiente condicionalmente** dado $T_2$.

Quando uma estat√≠stica √© anciliar, n√£o significa que ela n√£o tem import√¢ncia e, portanto, deveria ser ignorada. 
Significa que se ela fosse a √∫nica observa√ß√£o poss√≠vel, n√£o mudar√≠amos a informa√ß√£o sobre $\theta$. 
Podemos, todavia, alterar a informa√ß√£o sobre outras quantidades, todavia.

Uma estat√≠stica ancilar $U$ √© **maximal** se toda outra estat√≠stica ancilar √© fun√ß√£o de $U$.

**Teorema de Basu:** se $T$ √© uma estat√≠stica suficiente completa limitada e $U$ √© anciliar, ent√£o $U$ e $T$ s√£o independentes sob $P_{\theta}$ para qualquer $\theta$.
# Estat√≠stica suficiente

Um estat√≠stico usa a informa√ß√£o de uma amostra $X_1, \dots, X_n$ para fazer infer√™ncia sobre uma quantidade de interesse $\theta$.
De acordo com Fisher, em "On the Mathematical Foundations of Theoretical Statistics", "o objeto dos m√©todos estat√≠sticos √© a redu√ß√£o dos dados".
Como a quantidade de dados √© incapaz de ser compreendida diretamente pelo c√©rebro, ela √© reduzida por poucas quantidades que representam o todo, pelo menos a informa√ß√£o relevante para o problema. 
Nesse sentido, uma estat√≠stica √© uma forma de representar esses dados, em geral diminuir sua dimens√£o (os pr√≥prios dados formam uma estat√≠stica, ent√£o isso n√£o √© uma condi√ß√£o necess√°ria).
De forma precisa, a defini√ß√£o de **estat√≠stica** √©:

> Seja $(\mathcal{T}, \mathcal{C})$ um espa√ßo mensur√°vel em que $\mathcal{C}$ contenha todos os conjuntos unit√°rios formados pelo elementos de $\mathcal{T}$. Se $T : \mathcal{X} \to \mathcal{T}$ √© mensur√°vel, ent√£o $T$ √© uma estat√≠stica.

Com isso, podemos definir uma **estat√≠stica suficiente** no sentido cl√°ssico (lembrando que temos uma contrapartida bayesiana baseada na posteriori).

> Seja $T: \mathcal{X} \to \mathcal{T}$ uma estat√≠stica e $\mathcal{P}$ uma fam√≠lia de distribui√ß√µes parametrizada por $\theta$ e definida em $(\mathcal{X}, \mathcal{B})$ (espa√ßo amostral com $\sigma$-√°lgebra $\mathcal{B}$). Suponha que existem $P_{\theta}(\cdot|T)$ e uma fun√ß√£o $r : \mathcal{B} \times \mathcal{T} \to [0,1]$ tal que $r(\cdot, t)$ √© uma medida de probabilidade em $(\mathcal{X}, \mathcal{B})$ para todo $t \in \mathcal{T}$, $r(A, \cdot)$ √© mensur√°vel para todo $A \in \mathcal{B}$ e, para todo $\theta \in \Theta$ e $B \in \mathcal{B}$,
> $$
P_{\theta}(B|T=t) = r(B,t)
> $$
> quase certamente. Ent√£o dizemos que $T$ √© **estat√≠stica suficiente** para $\theta$.

Apesar dessa defini√ß√£o ser bastante complexa, a ideia √© que a distribui√ß√£o condicional de $X$ dado $T=t$ n√£o depende de $\theta$, isto √©, a informa√ß√£o trazida por $T=t$ sobre $\theta$ compreende toda a informa√ß√£o dispon√≠vel de $X$.
Ap√≥s observar $T(x) = t$, podemos amostrar de $r(\cdot, t)$ e teremos dados falsos que imitam os iniciais, pois a distribui√ß√£o ser√° a mesma.
Assim, se considerarmos dois experimentadores, um tendo a amostra inteira, e o outro tendo apenas uma estat√≠stica suficiente, ambos ter√£o a mesma quantidade de informa√ß√£o sobre o par√¢metro de interesse $\theta$.

Note que, pela lei da esperan√ßa total,
$$
P_{\theta}(X \in B) = \mathbb{E}_{\theta}[P_{\theta}(X \in B|T)] = \mathbb{E}_{\theta}[r(B, T)].
$$
Se tomarmos $Y \sim r(\cdot, t)$ quando $T=t$, teremos 
$$
P_{\theta}(Y \in B) = \mathbb{E}_{\theta}[P_{\theta}(Y \in B|T)] = \mathbb{E}_{\theta}[r(B, T)].
$$

**Proposi√ß√£o:** A estat√≠stica de ordem $(X_{(1)}, \dots, X_{(n)})$ √© estat√≠stica suficiente.

## Fatoriza√ß√£o de Fisher-Neyman

Seja $f(\boldsymbol{x}|\theta)$ a densidade de uma distribui√ß√£o de $\boldsymbol{X} = (X_1, \dots, X_n)$ (com respeito a uma medida $\nu$ $\sigma$-finita, como a medida de Lebesgue em $\mathbb{R}^n$). Ent√£o $T(\boldsymbol{X})$ √© estat√≠stica suficiente para $\theta$ se, e somente se, existem fun√ß√µes $m_1$ e $m_2$ tal que 
$$
f(\boldsymbol{x}|\theta) = m_1(\boldsymbol{x})m_2(T(\boldsymbol{x}) , \theta), \forall \theta \in \Theta.
$$

O lema 2.24 demonstrado no livro de Schervish determina que sob as hip√≥teses do teorema de Fisher-Neyman e assumindo que $T$ √© suficiente, obtemos que existe uma medida em $(\mathcal{T}, \mathcal{C})$ que domina a distribui√ß√£o de probabilidade de $T$ e define a densidade de $T=t$ sob o par√¢metro $\theta$ como $m_2(t,\theta)$.

## Estat√≠stica suficiente m√≠nima e completa

A estat√≠stica de ordem √© uma estat√≠stica que pouco reduz a informa√ß√£o do dado. 
De fato, s√≥ retira a quest√£o da ordena√ß√£o. 
Todavia, algumas vezes uma estat√≠stica mais simples tamb√©m √© suficiente e, por isso, faz sentido definir quando ela √© m√≠nima.

> Uma estat√≠stica suficiente $T$ √© dita **suficiente m√≠nima** se para toda estat√≠stica $U$, existe uma fun√ß√£o mensur√°vel $g$ tal que $T = g(U)$ para todo $\theta$.

**Teorema (Lehmann-Scheff√©):** Seja $f(x|\theta)$ a densidade e $T$ uma fun√ß√£o mensur√°vel tal que $T(x) = T(y) \iff y \in D(x)$, com
$$
D(x) = \{y \in \mathcal{X} : f(y|\theta) = f(x|\theta)h(x,y), \forall \theta \text{ e alguma fun√ß√£o } h(x,y) \}.
$$

---
``üìù`` **Exemplo**
}
Seja $X_1, \dots, X_n \overset{iid}{\sim} \operatorname{Bernoulli}(\theta)$. Temos que 
$$
f(x_1,\dots,x_n|\theta) = \theta^{S_x}(1-\theta)^{n-S_x},
$$
em que $S_x = \sum_{i=1}^n x_i$. 
Assim
$$
\frac{f(x_1,\dots,x_n|\theta)}{f(y_1,\dots,y_n|\theta)} = \left(\frac{\theta}{1-\theta}\right)^{S_x-S_y},
$$
que independe de $\theta$ se, e somente se, $S_x = S_y$. 
Isso mostra que $T(x) = S_x$ √© estat√≠stica suficiente m√≠nima.

---

---
``üìù`` **Exemplo - Fam√≠lia Exponencial**

Considere uma distribui√ß√£o com densidade 
$$
p_{\theta}(x) = \exp\{\eta(\theta)\cdot T(x) - B(\theta)\}h(x)
$$
com respeito a medida de Lebesgue. 
Pelo Teorema da Fatoriza√ß√£o $T(x)$ √© estat√≠stica suficiente.
Assim
$$
\log\left(\frac{p_{\theta}(y)}{p_{\theta}(x)}\right) = \eta(\theta)\cdot (T(y) - T(x)) + \log(h(y)) - \log(h(x)).
$$
Logo $T$ √© estat√≠stica suficiente m√≠nima se, e somente se, o complemento ortogonal de $\eta(\theta)$ possui apenas o $0$.

---

> Uma estat√≠stica $T$ √© dita **completa** se para toda fun√ß√£o $g$ mensur√°vel e $\theta \in \Theta$, $\mathbb{E}_{\theta}[g(T)] = 0$ implique $g(T) = 0$. Ela ser√° **completa limitada** se adicionamos a condi√ß√£o de que $g$ √© limitada.

**Teorema de Bahadur:** Se $U$ √© uma estat√≠stica suficiente completa limitada e de dimens√£o finita, ent√£o ela √© suficiente m√≠nima.

Ideias da demonstra√ß√£o:

- Tome uma outra estat√≠stica $T$ e defina $V_i(U) = (1 + \exp\{U_i\})^{-1}$ para cada componente de $U$. Note que $V$ √© limitada.
- Defina $H_i(t)$ como o valor esperado de $V_i$ dado que $T=t$ e $L_i(u)$ como o valor esperado de  $H_i(T)$ quando $U=u$, que n√£o dependem de $\theta$, pois as estat√≠sticas s√£o suficientes.
- Veja que $\mathbb{E}_{\theta}[V_i(U) - L_i(U)] = 0$, o que implica que $\mathbb{P}(V_i = L_i) = 1$ pois $U$ √© completa.
- Conclua usando a Lei da Vari√¢ncia Total que $U_i = V_i^{-1}(H_i(T))$.

Note que se $U$ √© completa, ent√£o $U$ √© completa limitada, o que nos d√° um resultado mais claro: Uma estat√≠stica suficiente e completa √© suficiente m√≠nima.

## Ancilaridade

Na outra ponta, temos estat√≠sticas que s√£o independentes do par√¢metro.

> Uma estat√≠stica $U$ √© dita anciliar se a sua distribui√ß√£o independe de $\theta$.

---
``üìù`` **Exemplo - Caso normal**

Sejam $X_1, X_2 \overset{iid}{\sim} \operatorname{Normal}(\mu, 1)$ e defina $U = X_2 - X_1$. Soma de normais independentes faz com que 
$U \sim \operatorname{Normal}(0, 2)$ e $U$ √© anciliar.

---

Se $T=(T_1, T_2)$ √© estat√≠stica suficiente m√≠nima e $T_2$ √© anciliar, dizemos que $T_1$ √© **suficiente condicionalmente** dado $T_2$.

Quando uma estat√≠stica √© anciliar, n√£o significa que ela n√£o tem import√¢ncia e, portanto, deveria ser ignorada. 
Significa que se ela fosse a √∫nica observa√ß√£o poss√≠vel, n√£o mudar√≠amos a informa√ß√£o sobre $\theta$. 
Podemos, todavia, alterar a informa√ß√£o sobre outras quantidades, todavia.

Uma estat√≠stica ancilar $U$ √© **maximal** se toda outra estat√≠stica ancilar √© fun√ß√£o de $U$.

**Teorema de Basu:** se $T$ √© uma estat√≠stica suficiente completa limitada e $U$ √© anciliar, ent√£o $U$ e $T$ s√£o independentes sob $P_{\theta}$ para qualquer $\theta$.

## Computacional 

Nesse notebook, que est√° dispon√≠vel [neste link](https://nbviewer.org/github/lucasmoschen/ta-sessions/blob/master/Statistical_Inference_MSc/Notebooks/Sufficient_Statistics.ipynb), faremos alguns poucos experimentos para ilustrar fatos verificados no cap√≠tulo de Estat√≠stica Suficiente.


```python
import numpy as np
import matplotlib.pyplot as plt
from scipy import stats
from tqdm import tqdm
```

### Amostrando!

Seja $X_1, \dots, X_n$ uma amostra aleat√≥ria de uma distribui√ß√£o $P_{\theta}$. 
Seja $T(X)$ uma estat√≠stica suficiente. 
Sabemos que $\mathbb{P}(X_1, \dots, X_n | T(X) = t)$ independe de $\theta$.
Suponha ent√£o que temos dois experimentadores. 
O primeiro captura todas as amostras e passa para o segundo experimentador apenas o valor de $T(X)$ para economizar!
Vamos verificar que o segundo experimentador consegue, agora, obter amostras fake da amostra original.

Vamos come√ßar com um exemplo simples: $X_1, \dots, X_n \sim \operatorname{Normal}(\mu, 1)$. Uma estat√≠stica suficiente (m√≠nima, inclusive) nesse caso √© $\bar{X} = n^{-1}(X_1 + \dots + X_n)$.

Sabemos que $\bar{X} \sim \operatorname{Normal}(\mu, n^{-1})$. Assim, 
$$
f(X_1, \dots, X_n | \bar{X} = t) = \frac{(2\pi)^{-n/2}\exp\{-1/2 \sum_{i=1}^n(x_i-\mu)^2\}}{(2\pi n^{-1})^{-1/2}\exp\{-n/2(t-\mu)^2\}} \propto \exp\left\{-\frac{1}{2}(S_x^2 - 2\mu n t + n\mu^2 - nt^2 + 2\mu n t - n\mu^2).\right\} = \exp\left\{-\frac{1}{2}\sum_{i=1}^n (x_i^2 - t^2)\right\},
$$
em que $S_x^2 = \sum_{i=1}^n x_i^2$ e $X_n = nt - \sum_{i=1}^{n-1} X_i$.

Note que podemos escrever 
$$
f(X_1, \dots, X_{n-1} | \bar{X} = t) \propto \exp\left\{nt^2/2\right\}\exp\left\{-\frac{1}{2}\left[\sum_{i=1}^{n-1} x_i^2 + \left(nt - \sum_{i=1}^{n-1} x_i\right)^2 \right]\right\} = \exp\left\{nt^2/2\right\}\exp\left\{-\frac{1}{2}\left[\sum_{i=1}^{n-1} x_i^2 + n^2t^2 - 2nt\sum_{i=1}^{n-1} x_i + \left(\sum_{i=1}^{n-1} x_i\right)^2\right]\right\}
$$

Assim, 
$$
\begin{split}
f(X_1, \dots, X_{n-1} | \bar{X} = t) &\propto \exp\left\{nt^2/2\right\}\exp\left\{-\frac{1}{2}\left[\sum_{i=1}^{n-1} (x_i^2 - 2nt x_i + nt^2) +nt^2 + \left(\sum_{i=1}^{n-1} x_i\right)^2\right]\right\} \\
&= \exp\left\{-\frac{1}{2}\left[\sum_{i=1}^{n-1} (x_i^2 - 2t x_i + t^2) - 2(n-1)t\sum_{i=1}^{n-1} x_i + (n-1)^2t^2 + \left(\sum_{i=1}^{n-1} x_i\right)^2\right]\right\} \\
&= \exp\left\{-\frac{1}{2}\left[\sum_{i=1}^{n-1} (x_i - t)^2 + \left(\sum_{i=1}^{n-1} (x_i - t)\right)^2 \right]\right\} \\
&= \exp\left\{-\frac{1}{2}\left[\sum_{i=1}^{n-1} (x_i - t)^2 + \sum_{j=1}^{n-1} \sum_{i=1}^{n-1} (x_j-t)(x_i - t) \right]\right\}
\end{split}
$$

Com isso, observamos que $X_1, \dots, X_n | \bar{X} = t$ tem distribui√ß√£o Normal Multivariada com $\mathbb{E}[X_i] = t$ para $i=1,\dots,n-1$
e matriz de covari√¢ncia (dada pela inversa) 
$$
\Sigma^{-1} = I_{n-1} + \begin{bmatrix}
1 & \cdots & 1 \\
\vdots & \ddots & \vdots \\
1 & \cdots & 1
\end{bmatrix} := I + A
$$

Veja que $\Sigma = I - A/n$ (mostre!). 

Assim, $X_1, \dots, X_{n-1} | \bar{X} = t \sim \operatorname{Normal}((t,\dots,t), I - A/n)$.


```python
mu = 2
n = 10
Sigma = np.eye(n-1) - np.ones((n-1,n-1))/n
```

Fa√ßamos um exemplo em que $\mu = 2$ verdadeiro (e desconhecido para os dois estat√≠sticos). Vamos fazer o seguinte experimento $M$ vezes, com $M = 100000$. Para o primeiro estat√≠stico, oferecemos $n$ amostras. Para o segundo estat√≠stico, s√≥ oferecemos a m√©dia dessas amostras e ele vai utilizar as contas acima para gerar outras $n$ amostras, isto, ele gerar√° amostras novas a partir de $X_1, \dots, X_{n-1} | \bar{X} = t \sim \operatorname{Normal}((t,\dots,t), I - A/n)$, pois ele conhecer√° $\bar{X}$. 

Como faremos esses experimentos diversas vezes, vamos obter $M$ amostras de $X_1$ para o estat√≠stico 1, e o estat√≠stico 2 vai produzir outras $M$ a partir da distribui√ß√£o calculada. Provamos que a distribui√ß√£o delas ser√° a mesma. Vamos verificar graficamente atrav√©s do histograma.

```python
mu = 2
n = 10
M = 100000

estatistico1 = np.zeros((M, n))
estatistico2 = np.zeros_like(estatistico1)

for k in tqdm(range(M)):
    x = np.random.normal(loc=mu, scale=1, size=n)
    t = x.mean()

    x_fake = np.random.multivariate_normal(mean=t*np.ones(n-1), cov=np.eye(n-1) - np.ones((n-1, n-1))/n)
    x_n = n * t - x_fake.sum()
    x_fake = np.hstack([x_fake, x_n])
    
    estatistico1[k] = x
    estatistico2[k] = x_fake
```

    100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100000/100000 [00:19<00:00, 5034.15it/s]

Veja que de fato os histogramas s√£o muito similares, como esper√°vamos! Assim, apenas tendo o valor da m√©dia, o estat√≠stico 2 foi capaz de produzir novas amostras a partir da mesma distribui√ß√£o do estat√≠stico 1!

```python
plt.hist(estatistico1[:,0], bins=50, label='Estat√≠stico 1')
plt.hist(estatistico2[:,0], alpha=0.5, bins=50, label='Estat√≠stico 2')
plt.title('Comparando a distribui√ß√£o do dado contra o dado falso')
plt.legend()
plt.show()
``` 
    
![png](output_9_0.png)

### Ancilaridade

Uma estat√≠stica anciliar n√£o depende do par√¢metro, ent√£o n√£o pode atualizar a informa√ß√£o sobre ele. 
Ser√° que significa que ela pode ser ignorada? 
Vamos verificar isso com um exemplo num√©rico. Seja $X_1, \dots, X_n \overset{iid}{\sim} \operatorname{Unif}[\theta-1/2, \theta+1/2]$. Assim 
$$
f(x_1, \dots, x_n | \theta) = \prod_{i=1}^n 1\{\theta -1/2 < x_i < \theta+1/2\} = 1\{\theta - 1/2 < \min\{x_i\}\}1\{\theta + 1/2 > \max\{x_i\}\}
$$
Portanto $T(X) = (\min(X_i), \max(X_i))$ √© estat√≠stica suficiente m√≠nima.
Seja $U(X) = \max(X_i) - \min(X_i)$.
Vamos verificar que $U$ √© estat√≠stica anciliar 

Defina $Y_i = X_i - \theta + 1/2 \sim \operatorname{Uniform}(0,1)$. Assim, 
$$
U = \max(X_i) - \min(X_i) = \max(X_i-\theta+1/2) - \min(X_i-\theta+1/2) = \max(Y_i) - \min(Y_i).
$$
Como a distribui√ß√£o de $Y$ independe de $\theta$, temos que a distribui√ß√£o de $U$ tamb√©m independe, o que mostra que $U$ √© estat√≠stica anciliar.

Vamos visualizar a distribui√ß√£o de $U$ de duas formas: partindo de $X$ e partido de $Y$.


```python
theta = 5
n = 10

X = np.random.uniform(theta-1/2, theta+1/2, size=(100000, n))
Y = np.random.uniform(size=(100000, n))

# Estamos tomando o m√°ximo de cada linha de X e obtendo 100000 amostras para X.
U1 = X.max(axis=1) - X.min(axis=1)
U2 = Y.max(axis=1) - Y.min(axis=1)
```

Note que de fato as distribui√ß√µes s√£o muito similares.


```python
plt.hist(U1,  bins=50, label='U = max(X_i) - min(X_i)')
plt.hist(U2, alpha=0.5, bins=50, label='U = max(Y_i) - min(Y_i)')
plt.title('Comparando a distribui√ß√£o de U')
plt.legend()
plt.show()
```
    
![png](output_13_0.png)
    
Se observamos que $U(X) = u$, podemos calcular que $\max(x_i) | U = u \sim \operatorname{Unif}(\theta-1/2+u, \theta+1/2)$. 
Assim, a distribui√ß√£o de uma estat√≠stica muda com outra anciliar.
